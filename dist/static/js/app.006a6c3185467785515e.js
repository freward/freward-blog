webpackJsonp([0],{"0U7u":function(t,a,e){"use strict";Object.defineProperty(a,"__esModule",{value:!0});var i={render:function(){var t=this.$createElement;return(this._self._c||t)("router-view")},staticRenderFns:[]},n=e("VU/8")(null,i,!1,null,null,null);a.default=n.exports},"2KgI":function(t,a){},"2wRn":function(t,a,e){"use strict";Object.defineProperty(a,"__esModule",{value:!0});var i={render:function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("section",[e("p",[e("vue-mathjax")],1),t._v(" "),e("h1",[t._v("I - Reinforcement Learning - from Policy Gradient to Deep Deterministic Policy Gradient")]),t._v(" "),t._m(0),t._v(" "),t._m(1),t._v(" "),e("p",[t._v("Here are the definitions of common terms in RL:")]),t._v(" "),t._m(2),t._v(" "),t._m(3),t._v(" "),e("br"),t._v(" "),e("p",[t._v("At a state $s$, agent interact with environment by action $a$,\nleading to a new state $s_{t+1}$ and receive a reward $r_{t+1}$.\nThe loop repeats like this until the final state reached $s_T$.")]),t._v(" "),e("h1",[t._v("1 - Example")]),t._v(" "),t._m(4),t._v(" "),t._m(5),t._v(" "),e("br"),t._v(" "),t._m(6),t._v(" "),e("h1",[t._v("2 - Policy Gradient")]),t._v(" "),e("p",[t._v("For a lively example, we examine a simple game problem, Hare Egg game.")]),t._v(" "),t._m(7),t._v(" "),e("br"),t._v(" "),e("p",[t._v("Let $\\pi_\\theta(a|s) = f(s, \\theta)$ is the policy of agent, it is a probability distribution of action $a$ at state $s$.")]),t._v(" "),t._m(8),t._v(" "),e("p",[t._v("Let $\\tau = s_1, a_1, s_2, a_2,..., s_T, a_T$ is the sequence from state $s_1$ to state $s_T$. The probability of $\\tau$ is likely to happen:")]),t._v(" "),e("p",[t._v("[\n\\begin{eqnarray}\np_\\theta(\\tau) &=& p_\\theta(s_1, a_1, s_2, a_2,...s_T, a_T) \\\\\n&=& p(s_1)\\pi_\\theta(a_1|s_1)p(s_2|s_1, a_1)\\pi_\\theta(a_2|s_2)...p(s_{T}|s_{T-1},a_{T-1})\\pi_\\theta(a_T|s_T) \\\\\n&=& p(s_1)\\Pi_{t=1}^{t=T}\\pi_\\theta(a_t|s_t)p(s_{t+1}|s_t, a_t) \\\\\n\\end{eqnarray}\n]")]),t._v(" "),e("p",[t._v("We will see that the probability distribution of state $p(s_{t+1}|a_t, s_t)$ will be eliminated later.")]),t._v(" "),t._m(9),t._v(" "),e("p",[t._v("$$\n\\begin{eqnarray}\n\\theta^* &=& \\arg\\max_\\theta E_{\\tau\\sim p_\\theta(\\tau)}\\big[r(\\tau)\\big] \\\\\n&=& \\arg\\max_\\theta E_{\\tau\\sim p_\\theta(\\tau)}\\bigg[\\sum_t r(a_t, s_t)\\bigg]\n\\end{eqnarray}\n$$")]),t._v(" "),t._m(10),t._v(" "),e("p",[t._v("$$\n\\begin{eqnarray}\nJ(\\theta) &=& E_{\\tau\\sim p_\\theta(\\tau)}\\bigg[\\sum_t r(a_t, s_t)\\bigg] \\\\\n&=& \\frac{1}{N} \\sum_i\\sum_t r(a_t, s_t)\n\\end{eqnarray}\n$$")]),t._v(" "),t._m(11),t._v(" "),e("p",[t._v("$$\n\\begin{eqnarray}\nJ(\\theta) &=& E_{\\tau\\sim p_\\theta(\\tau)}\\bigg[\\sum_t r(a_t, s_t)\\bigg] \\\\\n&=& \\int p_\\theta(\\tau) r(\\tau) dr\n\\end{eqnarray}\n$$")]),t._v(" "),e("p",[t._v("Continuing to examine gradient of the objective function:")]),t._v(" "),t._m(12),t._v(" "),e("p",[t._v("Take a closer look at $\\log p_\\theta(\\tau)$, as we have seen above $p_\\theta(\\tau) = p(s_1)\\Pi_{t=1}^{t=T}\\pi_\\theta(a_t|s_t)p(s_{t+1}|s_t, a_t)$, we have:\n$$\n\\begin{eqnarray}\n\\log p_\\theta(\\tau) = \\log p(s_1) + \\sum_{t=1}^{t=T}\\log \\pi_\\theta(a_t|s_t) + \\sum_{t=1}^{t=T}\\log p(s_{t+1}|s_t, a_t)\n\\end{eqnarray}\n$$")]),t._v(" "),t._m(13),t._v(" "),e("p",[t._v("Gradient of the objective function now becomes:\n$$\n\\begin{eqnarray}\n\\nabla_\\theta J(\\theta) &=&  E_{\\tau\\sim p_\\theta(\\tau)}\\bigg[\\nabla_\\theta \\log p_\\theta(\\tau) r(\\tau)\\bigg] \\\\\n&=& E_{\\tau\\sim p_\\theta(\\tau)}\\bigg[\\sum_{t=1}^{t=T}\\nabla_\\theta \\log\\pi_\\theta(a_t|s_t)\\sum_{t=1}^{t=T} r(a_t, s_t)\\bigg]\n\\end{eqnarray}\n$$")]),t._v(" "),e("p",[t._v("Similarly, after experiencing $N$ episodes, the expectation of this gradient is:\n$$\n\\begin{eqnarray}\n\\nabla_\\theta J(\\theta) &=& \\frac{1}{N}\\sum_{i=1}^{N}\\bigg(\\sum_{t=1}^{t=T}\\nabla_\\theta \\log \\pi_\\theta(a_{i,t}|s_{i,t})\\bigg)\\bigg(\\sum_{t=1}^{t=T} r(a_{i,t}, s_{i,t})\\bigg)\n\\end{eqnarray}\n$$")]),t._v(" "),e("p",[t._v("Finally, we update $\\theta$ using gradient ascent:\n$$\n\\begin{eqnarray}\n\\theta \\leftarrow \\theta + \\nabla_\\theta J(\\theta)\n\\end{eqnarray}\n$$")]),t._v(" "),e("h1",[t._v("3 - REINFORCE algorithm")]),t._v(" "),e("p",[t._v("Sum up all the result above, we have the REINFORCE algorithm as below:")]),t._v(" "),t._m(14),t._v(" "),e("p",[t._v("Now, let's stop to look closer on the gradient of the objective function. Write in a simple form, we have:")]),t._v(" "),t._m(15),t._v(" "),e("h1",[t._v("4 - Some new definitions")]),t._v(" "),t._m(16),t._v(" "),e("h2",[t._v("4.1 - Bellman Equations")]),t._v(" "),e("p",[t._v("From the formula above, we have:")]),t._v(" "),e("p",[t._v("$$\n\\begin{eqnarray}\nV^\\pi(s_t) &=& E_\\pi\\bigg[G_t|S=s_t\\bigg] \\\\\n&=& E_\\pi\\bigg[\\sum^{\\infty}_{k=0}\\gamma^kR_{k+t+1}|S=s_t\\bigg] \\\\\n\\end{eqnarray}\n$$")]),t._v(" "),e("p",[t._v("Take the reward $R_{t+1}$ received when going from state $s_t$ to $s_{t+1}$ to outside the $\\sum$, we get:")]),t._v(" "),e("p",[t._v("$$\n\\begin{eqnarray}\nE_\\pi\\bigg[R_{t+1} + \\gamma\\sum^{\\infty}_{k=0}\\gamma^kR_{k+t+2}|S=s_t\\bigg] &=& E_\\pi[R_{t+1}|S=s_t] + \\gamma E_\\pi\\bigg[\\sum^{\\infty}_{k=0}\\gamma^kR_{k+t+2}|S=s_t\\bigg]\n\\end{eqnarray}\n$$")]),t._v(" "),e("p",[t._v("Expand 2 expected values of the equation above, we have:")]),t._v(" "),e("p",[t._v("$$\n\\begin{eqnarray}\nE_\\pi\\bigg[R_{t+1}|S=s_t\\bigg]=\\sum_a \\pi(s_t,a) \\sum_{s_{t+1}} p(s_{t+1}|s_t, a)R(s_{t+1}|s_t, a)\n\\end{eqnarray}\n$$\nBut:")]),t._v(" "),e("p",[t._v("$$\n\\begin{eqnarray}\n\\gamma E_\\pi\\bigg[\\sum^{\\infty}_{k=0}\\gamma^kR_{k+t+2}|S=s_t\\bigg] = \\sum_a \\pi(s_t,a) \\sum_{s_{t+1}} p(s_{t+1}|s_t, a)\\gamma E_\\pi\\bigg[\\sum^\\infty_{k=0} \\gamma^k R_{t+k+2} | S = s_{t+1}\\bigg]\n\\end{eqnarray}\n$$\nWe have:\n$$\n\\begin{eqnarray}\nV^\\pi(s_t) = \\sum_a \\pi(s_t,a) \\sum_{s_{t+1}} p(s_{t+1}|s_t, a)\\Bigg[R(s_{t+1}|s_t, a) + \\gamma E_\\pi\\bigg[\\sum^\\infty_{k=0} \\gamma^k R_{t+k+2} | S = s_{t+1} \\bigg]\\Bigg]\n\\end{eqnarray}\n$$\nNotice that:\n$$\n\\begin{eqnarray}\nE_\\pi\\bigg[\\sum^\\infty_{k=0} \\gamma^k R_{t+k+2} | S = s_{t+1}\\bigg] = V^\\pi(s_{t+1})\n\\end{eqnarray}\n$$")]),t._v(" "),e("p",[t._v("Finally we have:\n$$\n\\begin{eqnarray}\nV^\\pi(s_t) = \\sum_a \\pi(s_t,a) \\sum_{s_{t+1}} p(s_{t+1}|s_t, a)\\bigg[R(s_{t+1}|s_t, a) + \\gamma  V^\\pi(s_{t+1})\\bigg]\n\\end{eqnarray}\n$$\nDoing similar thing with $Q^\\pi(s_t, a_t)$:\n$$\n\\begin{eqnarray}\nQ^\\pi(s_t, a_t) = \\sum_{s_{t+1}} p(s_{t+1}|s_t, a)\\bigg[R(s_{t+1}|s_t, a) + \\gamma \\sum_{a_{t+1}} \\pi(s_{t+1}, a_{t+1}) Q^\\pi (s_{t+1}, a_{t+1}) \\bigg]\n\\end{eqnarray}\n$$\nCombine with the relation between $V^\\pi$ and $Q^\\pi$ above, we have:\n$$\n\\begin{eqnarray}\n\\sum_{a_{t+1}} \\pi(s_{t+1}, a_{t+1}) Q^\\pi (s_{t+1}, a_{t+1}) = V^\\pi(s_{t+1})\n\\end{eqnarray}\n$$\nThus:\n$$\n\\begin{eqnarray}\nQ^\\pi(s_t, a_t) = \\sum_{s_{t+1}} p(s_{t+1}|s_t, a_t)\\bigg[R(s_{t+1}|s_t, a_t) + \\gamma  V^\\pi(s_{t+1}) \\bigg]\n\\end{eqnarray}\n$$")]),t._v(" "),e("p",[t._v("All of the above show that we can represent the value of $Q^\\pi$ and $V^\\pi$ at state $s_t$ with state $s_{t+1}$. Therefore, if we know the value at state $s_{t+1}$, we can easily calculate the value at state $s_t$. To sum up, we have 2 formulas below:\n$$\n\\begin{eqnarray}\nV^\\pi(s_t) &=& \\sum_a \\pi(s_t,a) \\sum_{s_{t+1}} p(s_{t+1}|s_t, a)\\bigg[R(s_{t+1}|s_t, a) + \\gamma  V^\\pi(s_{t+1})\\bigg] \\\\\nQ^\\pi(s_t, a_t) &=& \\sum_{s_{t+1}} p(s_{t+1}|s_t, a_t)\\bigg[R(s_{t+1}|s_t, a_t) + \\gamma  V^\\pi(s_{t+1}) \\bigg]\n\\end{eqnarray}\n$$")]),t._v(" "),e("p",[t._v("Going back with the gradient of the objective function, now we have:\n$$\n\\begin{eqnarray}\n\\nabla_\\theta J(\\theta) =  E_{\\tau\\sim p_\\theta(\\tau), a\\sim\\pi_\\theta}\\bigg[\\nabla_\\theta \\log\\pi_\\theta(a|s)Q^\\pi(s,a)\\bigg]\n\\end{eqnarray}\n$$")]),t._v(" "),e("h1",[t._v("5 - Advantage")]),t._v(" "),t._m(17),t._v(" "),t._m(18),t._v(" "),e("h1",[t._v("6 - Stochastic Actor-Critic")]),t._v(" "),t._m(19),t._v(" "),e("p",[t._v("Examine the gradient of the objective function that we have above:\n$$\n\\begin{eqnarray}\n\\nabla_\\theta J(\\theta) =  E_{\\tau\\sim p_\\theta(\\tau), a\\sim\\pi_\\theta}\\bigg[\\nabla_\\theta \\log\\pi_\\theta(a|s)\\Big(Q^\\pi(s,a)-V^\\pi(s)\\Big)\\bigg]\n\\end{eqnarray}\n$$\nFrom the Bellman Equation we have the relationship between $Q^\\pi$ and $V^\\pi$, now the objective function becomes:\n$$\n\\begin{eqnarray}\n\\nabla_\\theta J(\\theta) =  E_{\\tau\\sim p_\\theta(\\tau), a\\sim \\pi_\\theta}\\bigg[\\nabla_\\theta \\log\\pi_\\theta(a|s)\\Big(R + \\gamma V^\\pi(s_{t+1})- V^\\pi(s)\\Big)\\bigg]\n\\end{eqnarray}\n$$")]),t._v(" "),t._m(20),t._v(" "),e("h1",[t._v("7 - Actor-Critic Algorithm")]),t._v(" "),e("p",[t._v("From REINFORCE algorithm, now we use an additional approximation function for value function $V_\\phi$, changing a bit and we have:")]),t._v(" "),t._m(21),t._v(" "),e("h1",[t._v("8 - From Stochastic Actor-Critic to Q-Learning")]),t._v(" "),t._m(22),t._v(" "),t._m(23),t._v(" "),e("br"),t._v("\nTherefore, with a policy $\\pi$, we always can apply policy $\\pi'$ over it to have a new policy equal or better."),e("br"),t._v("\nWe now have the algorithm as follow:"),e("br"),t._v("\n1. Evaluate $A^\\pi(s,a)$ with different actions $a$ "),e("br"),t._v("\n2. Optimize $\\pi \\leftarrow \\pi'$\n"),t._m(24),t._v(" "),t._m(25),t._v(" "),e("p",[t._v("Now we actually do not need to care about policy anymore, and the second step can be written as:")]),t._v(" "),t._m(26),t._v(" "),e("p",[t._v("If we use an approximation function for $V_\\phi(s)$, we have the following algorithm:")]),t._v(" "),t._m(27),t._v(" "),e("p",[t._v("This algorithm is not good, in the first step we need to have reward $r(s, a)$ corresponding to different actions $a$, thus we need different simulations at a state $s$. To solve this, we could do the same analysis above with $Q(s,a)$ instead of $V(s)$.")]),t._v(" "),t._m(28),t._v(" "),t._m(29),t._v(" "),t._m(30),t._v(" "),t._m(31),t._v(" "),e("p",[t._v("To sum up, for the algorithm to be stable, and possibly converge, we need:")]),t._v(" "),t._m(32),t._v(" "),e("p",[t._v("The algorithm now:")]),t._v(" "),t._m(33),t._v(" "),e("h1",[t._v("9 - From Deep Q-Network to Deep Deterministic Policy Gradient")]),t._v(" "),e("p",[t._v("DQN algorithm is succeeded to approximate Q-value, but there is a limitation in step 3: we need to evaluate $Q_{\\phi'}$ with all different actions to choose the highest $Q$. With discrete action space such as games, when the set of actions is just up down left right buttons, the number of actions is finite and small, this is possible. However, in continuous action space, for example with action in a range from 0 to 1 we need a different approach.\nThe first thing we could think of is to discretize the action space into bins, for example from 0 to 1, we could divide it by 5 or 10 bins. Another way is that we sample actions with uniform distribution on the action space and choose the highest $Q(s,a)$ at state $s$.")]),t._v(" "),t._m(34),t._v(" "),e("p",[t._v("Now, if we add an approximation function $\\mu_\\theta(s) = \\arg max_a Q_\\phi(s,a)$, we will have to find $\\theta$ in which: $\\theta \\leftarrow  \\arg max_\\theta Q_\\phi(s,\\mu_\\theta(s))$. This optimization evaluates the change of $Q_\\phi$ w.r.t parameters $\\theta$. We could evaluate this change by the chain rule as follow:\n$\\frac{dQ_\\phi}{d\\theta} = \\frac{dQ_\\phi}{d\\mu} \\frac{d\\mu}{d\\theta}$.")]),t._v(" "),e("p",[t._v("Notice that, $\\mu_\\theta(s)$ is a Deterministic Policy, therefore this method is called Deep Deterministic Policy Gradient.")]),t._v(" "),e("p",[t._v("The DDPG algorithm is as follow:")]),t._v(" "),t._m(35),t._v(" "),t._m(36),t._v(" "),e("h1",[t._v("10 - Conclusion")]),t._v(" "),e("p",[t._v("To sum up, we went throught the Policy Gradient algorithm to DQN and DDPG.")]),t._v(" "),e("p",[t._v("Take away:")]),t._v(" "),t._m(37)])},staticRenderFns:[function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("p",[t._v("Reinforcement Learning is the field related to teaching machine "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("agent"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(" to perform well a task by interacting with the environment and receive rewards.\nThis way of learning is very similar to the way human learns from the environment by the wrong test method. Taking an example of a child in the winter, the child will tend to get closer to the fire "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("because the reward is warm"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(", but touching the fire is hot, the child will tend to avoid touching the fire "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("it will burn him"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(".")])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("In the above example, the reward appears immediately, the action adjustment is\nrelatively easy. However, in more complex situations where rewards are far in the future, this becomes more complicated. How to achieve highest accumulative reward throughout the process? Reinforcement Learning algorithms"),a("span",{staticClass:"tex2jax_ignore"},[this._v("(")]),this._v(" RL"),a("span",{staticClass:"tex2jax_ignore"},[this._v(")")]),this._v(" is to solve this optimization problem.")])},function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("ul",[e("li",[e("em",[t._v("Environment")]),t._v(" : is the space, the game, the environment that the machine interacts with.")]),t._v(" "),e("li",[e("em",[t._v("Agent")]),t._v(" : the machine observes the environment and generates action accordingly.")]),t._v(" "),e("li",[e("em",[t._v("Policy")]),t._v(": the rule which the agent follows to get the goal.")]),t._v(" "),e("li",[e("em",[t._v("Reward")]),t._v(": a reward that the agent received from the environment for taking an action.")]),t._v(" "),e("li",[e("em",[t._v("State")]),t._v(" : the state of the environment that the agent observes.")]),t._v(" "),e("li",[e("em",[t._v("Episode")]),t._v(" : the sequence of state and action until it finishes $s_1,a_1,s_2,a_2,...s_T, a_T$")]),t._v(" "),e("li",[e("em",[t._v("Accumulative Reward")]),t._v(" : the sum of all rewards received from an episode.")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("div",{staticStyle:{width:"image width px","font-size":"80%","text-align":"center"}},[a("img",{attrs:{src:"https://i.imgur.com/nIUdsIm.jpg",align:"center"}}),this._v(" "),a("div",[this._v("Image 1: The interaction loop between agent and environment.")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("This example below is from openAI Gym, environment named\n"),a("a",{attrs:{href:"https://github.com/openai/gym/wiki/MountainCarContinuous-v0"}},[this._v("MountaincontinuousCar-v0")]),this._v(".")])},function(){var t=this.$createElement,a=this._self._c||t;return a("div",{staticStyle:{width:"image width px","font-size":"80%","text-align":"center"}},[a("img",{staticStyle:{"padding-bottom":"0.5em"},attrs:{src:"https://i.imgur.com/yGWmDei.jpg",alt:"MountaincontinuousCar-v0"}}),this._v(" "),a("div",[this._v("Image 2: A render from MountaincontinuousCar-v0.")])])},function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("ul",[e("li",[e("em",[t._v("Goal")]),t._v(" : the goal of this game is to find a policy to control the car reaching the flag.")]),t._v(" "),e("li",[e("em",[t._v("Environment")]),t._v(" : ramps and cars running in it.")]),t._v(" "),e("li",[e("em",[t._v("State")]),t._v(" : the state of the vehicle has 2 dimensions, the coordinates of the vehicle in the $x$ axis and the speed of the vehicle at the time of measurement.")]),t._v(" "),e("li",[e("em",[t._v("Action")]),t._v(" : Force applied to control the vehicle, however the force is not strong enough in order to push the car at once to the flag. The car will need to go back and forth on the sides of gain enough acceleration and reach the flag.")]),t._v(" "),e("li",[e("em",[t._v("Reward")]),t._v(" : With every step that the car cannot reach the flag, the agent gets a reward $r=\\frac{-a^2}{10}$,\nand a reward 100 if it reach the target. Thus, if the agent controls the car but it cannot reach the flag, the agent will be punished.")]),t._v(" "),e("li",[e("em",[t._v("Terminal state")]),t._v(" : ff the agent reaches the flag or the step number exceeds 998 steps.")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("div",{staticStyle:{width:"image width px","font-size":"80%","text-align":"center"}},[a("img",{attrs:{src:"https://laptrinhcuocsong.com/images/game-hung-trung.png",align:"center"}}),this._v(" "),a("div",[this._v("Image 3: Hare Egg game.")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("In Hare Egg game, assume that we have 3 actions: go left, go right or stand still.\nCorresponding to the current state $s$ "),a("span",{staticClass:"tex2jax_ignore"},[this._v("(")]),this._v("the position of the basket, the position of the egg falling against the basket,\nspeed of eggs falling..."),a("span",{staticClass:"tex2jax_ignore"},[this._v(")")]),this._v(" we will have a probability distribution of action,\nfor example $[0.1, 0.3, 0.5]$. The sum of all action probability at state $s$ is $1$, we have: $\\sum_{a}\\pi_\\theta(a|s) = 1$.\nLet $p(s_{t+1}|a_t, s_t)$ is the probability distrubution of the next state when the agent is at state $s$ and executes an action $a$.")])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[a("strong",[this._v("The goal of reinforcement learning is to find $\\theta$ such that:")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("From the formula we can see $\\theta^*$ is a set of parameters such that the expectation of accumulative reward from many different samples $\\tau$, which we collect by the current policy $\\pi_\\theta$ is biggest."),a("br"),this._v("\nAfter $N$ different episodes, the agent collects $N$ different samples $\\tau$. The objective function now becomes:")])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("$J(\\theta)$ is the average of accumulative reward from $N$ episodes."),a("br"),this._v("\nWe can also see $J(\\theta)$ under the probability distribution $p_\\theta(\\tau)$ as below:")])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("$$\n\\begin{eqnarray}\n\\nabla_\\theta J(\\theta) &=& E_{\\tau\\sim p_\\theta(\\tau)}\\bigg[\\sum_t r(a_t, s_t)\\bigg] \\\\\n&=& \\int \\nabla_\\theta p_\\theta(\\tau) r(\\tau) dr\n\\end{eqnarray}\n$$\nBut we also have:\n$$\n\\begin{eqnarray}\n\\nabla_\\theta p_\\theta(\\tau) &=&  p_\\theta(\\tau) \\frac{\\nabla_\\theta p_\\theta(\\tau)} {p_\\theta(\\tau)} \\\\\n&=& p_\\theta(\\tau)\\nabla_\\theta \\log p_\\theta(\\tau)\n\\end{eqnarray}\n$$\n"),a("strong",[this._v("Note")]),this._v(" that this trick is usually used, thus:\n$$\n\\begin{eqnarray}\n\\nabla_\\theta J(\\theta) &=& \\int p_\\theta(\\tau) \\nabla_\\theta \\log p_\\theta(\\tau) r(\\tau) dr \\\\\n&=& E_{\\tau\\sim p_\\theta(\\tau)}\\bigg[\\nabla_\\theta \\log p_\\theta(\\tau) r(\\tau)\\bigg]\n\\end{eqnarray}\n$$")])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("Finally:\n$$\n\\begin{eqnarray}\n\\nabla_\\theta \\log p_\\theta(\\tau) = \\sum_{t=1}^{t=T}\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\n\\end{eqnarray}\n$$\nThis result is beautiful because the derivative with respect to "),a("span",{staticClass:"tex2jax_ignore"},[this._v("(")]),this._v("w.r.t"),a("span",{staticClass:"tex2jax_ignore"},[this._v(")")]),this._v(" $\\theta$ of the function $\\log p_\\theta(\\tau)$ is no longer dependent on the transition probability of state $p(s_{t+1}|a_t, s_t)$, it is only dependent on the probability distribution of action $a_i$ which the agent execute on $s_i$.")])},function(){var t=this.$createElement,a=this._self._c||t;return a("ol",[a("li",[this._v("Collect $N$ samples {$\\tau^i$} with the policy $\\pi_\\theta$")]),this._v(" "),a("li",[this._v("Calculate gradient: $\\nabla_\\theta J(\\theta) = \\frac{1}{N}\\sum_{i=1}^{N}\\bigg(\\sum_{t=1}^{t=T}\\nabla_\\theta \\log \\pi_\\theta(a_{i,t}|s_{i,t})\\bigg)\\bigg(\\sum_{t=1}^{t=T} r(a_{i,t}, s_{i,t})\\bigg) $")]),this._v(" "),a("li",[this._v("Update $\\theta \\leftarrow \\theta + \\nabla_\\theta J(\\theta)$")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("$$\n\\begin{eqnarray}\n\\nabla_\\theta J(\\theta) = \\frac{1}{N}\\sum_{i=1}^{N}\\nabla_\\theta \\log \\pi_\\theta(\\tau_i)r(\\tau_i)\n\\end{eqnarray}\n$$\nThis is exactly the maximum likelihood estimation "),a("a",{attrs:{href:"https://en.wikipedia.org/wiki/Maximum_likelihood_estimation"}},[this._v("MLE")]),this._v(" multiply with the accumulative reward.\nOptimizing the objective function also means increasing the probability to follow sequences $\\tau$ which give high accumulative rewards.")])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("$V^\\pi(s)$: expected accumulative reward at state $s$ by policy $\\pi$."),a("br"),this._v("\n$Q^\\pi(s,a)$: expected accumulative reward if execute action $a$ at state $s$ by policy $\\pi$."),a("br"),this._v("\nThe relationship between $V^\\pi(s)$ and $Q^\\pi(s,a)$: $V^\\pi(s) = \\sum_{a \\in A}\\pi_\\theta(s,a)Q^\\pi(s,a)$ - this makes sense because $\\pi_\\theta(s,a)$ is the probability of doing action $a$ at state $s$."),a("br"),this._v("\nTWe also have:\n$$\n\\begin{eqnarray}\nV^\\pi(s_t) &=& E_\\pi[G_t | S=s_t] \\\\\nQ^\\pi(s_t,a_t) &=& E_\\pi[G_t|S=s_t, A=a_t]\n\\end{eqnarray}\n$$\nIn which:"),a("br"),this._v("\n$G_t=\\sum^{\\infty}_{k=0}\\gamma^kR_{k+t+1}$: sum of all reward will be received from state $s_t$ to the future, with $\\gamma$ called discount factor: $0 < \\gamma < 1$. The farther into the future, the reward will be discounted more, agent cares more about incoming rewards than far future rewards.")])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("$$\n\\begin{eqnarray}\n\\nabla_\\theta J(\\theta) =  E_{\\tau\\sim p_\\theta(\\tau), a\\sim\\pi_\\theta}\\bigg[\\nabla_\\theta \\log\\pi_\\theta(a|s)Q^\\pi(s,a)\\bigg]\n\\end{eqnarray}\n$$\nGradient of the objective function shows that the agent will do more action $a$ if it receives a high $Q^\\pi(s,a)$. Assuming that the agent is at state $s$, the fact that it is at state $s$ is already good for the agent, executing any action $a$ will give back a high $Q^\\pi(s,a)$ so it cannot discriminate its actions $a$ and from there, it does not know which action  $a$ is optimal. Therefore, we need a baseline to compare the value of $Q^\\pi(s,a)$."),a("br"),this._v("\nAs in the part 4, we have $V^\\pi(s)$ is the expectation of accumulative reward at state $s$, no matter what action the agent will take at state $s$, we expect an accumulative reward $V^\\pi(s)$ from there to the end.\nTherefore, an action $a_m$ is bad if $Q^\\pi(s,a_m)$ < $V^\\pi(s)$ and an action $a_n$ is good if $Q^\\pi(s,a_n)$ > $V^\\pi(s)$. From here we have 1 baseline to compare $Q^\\pi(s,a)$ which is $V^\\pi(s)$. The gradient of objective function now can be written:\n$$\n\\begin{eqnarray}\n\\nabla_\\theta J(\\theta) =  E_{\\tau\\sim p_\\theta(\\tau), a\\sim\\pi_\\theta}\\bigg[\\nabla_\\theta \\log\\pi_\\theta(a|s)\\Big(Q^\\pi(s,a)-V^\\pi(s)\\Big)\\bigg]\n\\end{eqnarray}\n$$")])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("If $Q^\\pi(s,a)-V^\\pi(s) < 0$, 2 gradients have opposite signs, optimizing the objective function will decrease the probability of executing action $a$ at $s$."),a("br"),this._v("\nWe call $A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)$ is the Advantage of action $a$ at state $s$.")])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("Stochastic Actor means the policy $\\pi_\\theta(a|s)$ is a probability distribution of actions at $s$. We call Stochastic Actor to distinguish it from Deterministic Actor "),a("span",{staticClass:"tex2jax_ignore"},[this._v("(")]),this._v("or Deterministic Policy"),a("span",{staticClass:"tex2jax_ignore"},[this._v(")")]),this._v(" which means the policy is not a probability distribution of actions at $s$, but under $s$ we only execute a deterministic action, in another word, the probability execution of a chosen action $a=\\mu_\\theta(s)$ under $s$ is 1 and all other actions is 0.")])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("The objective function depends on 2 things: policy $\\pi_\\theta$ and value function $V^\\pi$. Assuming that we have an approximation function for $V^\\pi(s)$ is $V_\\phi(s)$ depending on parameters $\\phi$."),a("br"),this._v("\nWe call the appromixation function for policy $\\pi_\\theta$ is Actor and the appromixation function for $V_\\phi$ is Critic.")])},function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("p",[t._v("Batch Actor-Critic:"),e("br"),t._v("\n1. Sample a rollout $\\tau$ to terminal state by the policy $\\pi_\\theta$"),e("br"),t._v("\n2. Fit $V_\\phi$ avec $y = \\sum_{i}^{T} r_i$"),e("br"),t._v("\n3. Find $A(s_t,a_t) = r(s_t, a_t) + \\gamma V_\\phi(s_{t+1}) - V_\\phi(s_{t})$"),e("br"),t._v("\n4. Find $\\nabla_\\theta J(\\theta) = \\sum_i \\nabla \\log \\pi_\\theta (a_i|s_i) A^\\pi (s_i, a_i)$"),e("br"),t._v("\n5. Update $\\theta \\leftarrow \\theta  + \\alpha \\nabla_\\theta J(\\theta)$"),e("br"),t._v(" "),e("br"),t._v(" "),e("br"),t._v("\nAbove, we can represent $V_\\phi(s) = r + V_\\phi(s')$ according to Bellman Equation, therfore we could update the model knowing only 1 step ahead."),e("br"),t._v("\nOnline Actor-Critic:"),e("br"),t._v("\n1. With policy $\\pi_\\theta$, execute 1 action $a \\sim \\pi_\\theta(a|s)$ to have $(s,a,s',r)$"),e("br"),t._v("\n2. Fit $V_\\phi (s)$ avec $r + V_\\phi(s')$"),e("br"),t._v("\n3. Find $A(s_t,a_t) = r(s_t, a_t) + \\gamma V_\\phi(s_{t+1}) - V_\\phi(s_{t})$"),e("br"),t._v("\n4. Find $\\nabla_\\theta J(\\theta) = \\sum_i \\nabla \\log \\pi_\\theta (a_i|s_i) A (s_i, a_i)$"),e("br"),t._v("\n5. Update $\\theta \\leftarrow \\theta  + \\alpha \\nabla_\\theta J(\\theta)$"),e("br"),t._v(" "),e("br"),t._v(" "),e("br"),t._v("\nThus, we update interatively both approximation functions $V_\\phi$ and $\\pi_\\theta$.")])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("Examine a policy as follow:\n$$\n\\begin{eqnarray}\n\\pi'(a_t|s_t) = 1 \\ \\text{if}\\  a_t = \\arg \\max_{a_t} A^\\pi(s_t, a_t)\n\\end{eqnarray}\n$$\nPolicy $\\pi'$ is a Deterministic Policy: given a policy $\\pi$ and assuming we know the Advantage of actions at state $s_t$ under policy $\\pi$, we always choose the action with the highest Advantage at state $s$, probability of that action is 1, all other actions at $s_t$ is 0.\nPolicy $\\pi'$ will be always better or at least equal to policy $\\pi$. A policy is evaluated as equal or better than other if:\n$V^\\pi(s) \\leq V^{\\pi'} (s) \\forall s \\in S$ : with all state $s$ in the state space $S$, the return value $V^\\pi(s)$ always less than or equal to the return value $V^{\\pi'} (s)$."),a("br"),this._v("\nFor example, we have: at state $s$, we have 4 ways to go from state $s'$ corresponding to 4 actions and Advantages $A^\\pi_1$, $A^\\pi_2$, $A^\\pi_3$, $A^\\pi_4$. From state $s'$, we continue to follow policy $\\pi$. From $s$ to $s'$, if we choose to follow stochastic policy $\\pi$, expected Advantage is $\\sum_{a \\in A} p(a)A^\\pi_a$, this quantity must be less than or equal to $\\max_a A^\\pi_a$.")])},function(){var t=this.$createElement,a=this._self._c||t;return a("div",{staticStyle:{width:"image width px","font-size":"80%","text-align":"center"}},[a("img",{attrs:{src:"https://i.imgur.com/yMtTahR.jpg",align:"center"}}),this._v(" "),a("div",[this._v("Image 4: Transition from state $s$ to $s'$.")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("But evaluating $A^\\pi(s,a)$ is also equivalent to evaluate $Q^\\pi(s,a)$ because $A^\\pi(s,a) =  Q^\\pi(s,a) - V^\\pi(s) = r(s,a)  + \\gamma V^\\pi(s') - V^\\pi(s)$, and the quantity $V^\\pi(s)$ is the same for different actions $a$ at state $s$."),a("br"),this._v("\nThe algorithm now becomes:")])},function(){var t=this.$createElement,a=this._self._c||t;return a("ol",[a("li",[this._v("Evaluate $Q^\\pi(s,a) \\leftarrow r(s,a)  + \\gamma V^\\pi(s') $ with different actions $a$")]),this._v(" "),a("li",[this._v("Optimize $\\pi \\leftarrow \\pi'$ : choose the action with highest $A$, it is also highest $Q$")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("ol",[a("li",[this._v("Evaluate $Q^\\pi(s,a) \\leftarrow r(s,a)  + \\gamma V^\\pi(s') $ for different actions $a$")]),this._v(" "),a("li",[this._v("$V^\\pi(s) \\leftarrow \\max_a Q^\\pi(s,a)$")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("ol",[a("li",[this._v("Evaluate $V^\\pi(s) \\leftarrow \\max_a \\big(r(s,a)  + \\gamma V^\\pi(s')\\big)$")]),this._v(" "),a("li",[this._v("$\\phi \\leftarrow \\arg min_\\phi \\big(V^\\pi(s) - V_\\phi(s)\\big)^2$")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("ol",[a("li",[this._v("Evaluate $y_i \\leftarrow r(s,a_i)  + \\gamma \\max_{a'} Q_\\phi(s', a') $")]),this._v(" "),a("li",[this._v("$\\phi \\leftarrow \\arg min_\\phi \\big(Q_\\phi(s, a_i) - y_i\\big)^2$ $(*)$")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("This is the Q-Learning algorithm. Notice that, reward $r$ above is not dependent on the state transition and on the policy $\\pi$ used to generate the sample, therefore we only need the sample $(s, a, r, s')$ to improve the policy without knowing that sample is generated from which policy. Because of this reason, we call it off-policy. Later, we will have on-policy algorithms and they need the new samples generated by the current policy to improve itself "),a("span",{staticClass:"tex2jax_ignore"},[this._v("(")]),this._v("cannot use experience from history"),a("span",{staticClass:"tex2jax_ignore"},[this._v(")")]),this._v(".\nWe have the Online Q-Learning as follow:")])},function(){var t=this.$createElement,a=this._self._c||t;return a("ol",[a("li",[this._v("Evaluate action $a$ to have $(s, a, s', r)$")]),this._v(" "),a("li",[this._v("Evaluate $y_i \\leftarrow r(s,a_i)  + \\gamma max_{a'} Q_\\phi(s', a') $")]),this._v(" "),a("li",[this._v("$\\phi \\leftarrow \\phi - \\alpha \\frac{dQ_\\phi}{d\\phi}(s,a) \\big(Q_\\phi(s, a_i) - y_i\\big)$")])])},function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("p",[t._v("Notice in the step 3, is it the gradient descent as same as where I marked $(*)$ above? The answer is no, in fact, we ignore the derivative of $y_i$ by $\\phi$ "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("$y_i$ also depends on $\\phi$"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(". As a consequence, everytime we update $\\phi$ with this algorithm, the value of the target $y_i$ is also changed! The target changes when we try to get closer, this makes the algorithm becomes unstable."),e("br"),t._v("\nTo solve this, we need another approximation function called target network, different from the train network we use to run. Target network will be update slowly and is used to evaluate $y$."),e("br"),t._v("\nAnother problem is that samples are generated continuously so they are correlated. The algorithm above is similar as Supervised Learning - we map a Q-value with a target, we want the samples are independent and identically distributed "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("i.i.d"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(". To break the correlation between samples, we could use an experience buffer: a list containing many samples from different episodes and we choose randomly a batch from the buffer and train our agent on that batch.")])},function(){var t=this.$createElement,a=this._self._c||t;return a("ul",[a("li",[this._v("A separated target network called $Q_{\\phi'}$.")]),this._v(" "),a("li",[this._v("Experience buffer.")])])},function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("ol",[e("li",[t._v("Execute action $a_i$ to have $(s_i, a_i, s_i', r_i)$ and put it into the buffer.")]),t._v(" "),e("li",[t._v("Sample randomly a batch $N$ samples from the buffer $(s_i, a_i, s_i', r_i)$.")]),t._v(" "),e("li",[t._v("Evaluate $y_i \\leftarrow r(s,a_i)  + \\gamma max_{a'} Q_{\\phi'}(s', a')$ "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("using target network here"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")])]),t._v(" "),e("li",[t._v("$\\phi \\leftarrow \\phi - \\alpha \\frac{1}{N}\\sum_i^N \\frac{dQ_\\phi}{d\\phi}(s,a_i) \\big(Q_\\phi(s, a_i) - y_i\\big)$")]),t._v(" "),e("li",[t._v("Update target network $\\phi' \\leftarrow (1-\\tau)\\phi' + \\tau \\phi$ "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("using $\\tau %$ of new train network to update target network"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(" "),e("br"),t._v(" "),e("br"),t._v("\nThis algorithm is Deep Q-Network "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("DQN"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(".")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("Deep Deterministic Policy Gradient "),a("span",{staticClass:"tex2jax_ignore"},[this._v("(")]),this._v("DDPG"),a("span",{staticClass:"tex2jax_ignore"},[this._v(")")]),this._v(" has a nice approach, notice that:\n$$\n\\begin{eqnarray}\nmax_{a} Q_{\\phi}(s, a) = Q_\\phi\\big(s, \\arg max_a Q_\\phi(s,a)\\big)\n\\end{eqnarray}\n$$")])},function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("ol",[e("li",[t._v("Execute action $a_i$ to have $(s_i, a_i, s_i', r_i)$ and put it into the buffer.")]),t._v(" "),e("li",[t._v("Sample a batch of $N$ samples from the buffer $(s_i, a_i, s_i', r_i)$.")]),t._v(" "),e("li",[t._v("Evaluate $y_i \\leftarrow r(s,a_i)  + \\gamma Q_{\\phi'}\\big(s', \\mu_{\\theta'}(s')\\big)$ "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("use both policy and Q target network here"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")])]),t._v(" "),e("li",[t._v("$\\phi \\leftarrow \\phi - \\alpha \\frac{1}{N}\\sum_i^N \\frac{dQ_\\phi}{d\\phi}(s,a_i) \\big(Q_\\phi(s, a_i) - y_i\\big)$")]),t._v(" "),e("li",[t._v("$\\theta \\leftarrow \\theta - \\beta \\frac{1}{N}\\sum_i^N \\frac{d\\mu_\\theta}{d\\theta}(s) \\frac{dQ_\\phi}{da}(s, a_i)$")]),t._v(" "),e("li",[t._v("Update target network $\\phi' \\leftarrow (1-\\tau)\\phi' + \\tau \\phi$ and $\\theta' \\leftarrow (1-\\tau)\\theta' + \\tau \\theta$\n"),e("br"),t._v(" "),e("br"),t._v("\nNotice in the  DDPG implementation:")])])},function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("ul",[e("li",[t._v("Because actions in DDPG are always deterministic, thus to explore the environment "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("we do not want the agent always exploit the best trajectory in its knowledge, there may be a better path out there"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(", a small action noise will be added into the action from the agent.\nThe noise in the "),e("a",{attrs:{href:"https://arxiv.org/abs/1509.02971"}},[t._v("original paper")]),t._v(" is a stochastic process named Ornstein–Uhlenbeck process "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("OU process"),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v(". The authors choosed this process because the experiments gave good result, however other experiments conducted by different groups showing that other noise such as Gaussian Noise give the same performance.")]),t._v(" "),e("li",[t._v("Implementation of action noise in the library "),e("a",{attrs:{href:"https://github.com/keras-rl/keras-rl"}},[t._v("keras-rl")]),t._v(" is OU process, however when I run, this noise is not decayed w.r.t time. Hoever, we need a big noise at the beginning "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("to explore the environment"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(" and decrease after many episodes. This can be done if before we add the noise to the action, we multiply it with a quantity epsilon and the epsilon decays to 0 w.r.t time.")]),t._v(" "),e("li",[t._v("Besides adding noise into the action before executing it on the environment, they also add Gaussian Noise on the node of Neural Network. Reference paper "),e("a",{attrs:{href:"https://openai.com/blog/better-exploration-with-parameter-noise/"}},[t._v("here")]),t._v(". You could find the implementation o actor network noise in this library "),e("a",{attrs:{href:"https://stable-baselines.readthedocs.io/en/master/modules/ddpg.html#action-and-parameters-noise"}},[t._v("stable baselines")]),t._v(".")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("ul",[a("li",[this._v("Policy Gradient is an on-policy and a stochastic policy.")]),this._v(" "),a("li",[this._v("Q-learning, DQN, DDPG are off-policy and deterministic policies.")]),this._v(" "),a("li",[this._v("Always have a deterministic policy better than a stochastic policy.")])])}]},n=e("VU/8")(null,i,!1,null,null,null);a.default=n.exports},"4jvi":function(t,a,e){"use strict";Object.defineProperty(a,"__esModule",{value:!0});var i={components:{SideMenu:e("9fpD").a}},n={render:function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("div",[e("nav",{staticClass:"navbar is-fixed-top is-light is-bold"},[t._m(0),t._v(" "),e("div",{staticClass:"navbar-menu",attrs:{id:"navbarLanguage"}},[e("div",{staticClass:"navbar-start"},[e("div",{staticClass:"navbar-item has-dropdown is-hoverable"},[e("a",{staticClass:"navbar-link"},[t._v("\n                      Ngôn Ngữ\n                  ")]),t._v(" "),e("div",{staticClass:"navbar-dropdown is-boxed"},[e("a",{staticClass:"navbar-item"},[e("router-link",{attrs:{to:"/"}},[e("span",[t._v("Tiếng Anh")])])],1),t._v(" "),e("a",{staticClass:"navbar-item"},[e("router-link",{attrs:{to:"/vi"}},[e("span",[t._v("Tiếng Việt")])])],1),t._v(" "),e("a",{staticClass:"navbar-item"},[e("router-link",{attrs:{to:"/fr"}},[e("span",[t._v("Tiếng Pháp")])])],1)])])]),t._v(" "),t._m(1)])]),t._v(" "),e("section",{staticClass:"section"},[e("div",{staticClass:"container"},[e("div",{staticClass:"columns"},[e("div",{staticClass:"column is-2 side-nav"},[e("side-menu")],1),t._v(" "),e("div",{staticClass:"column is-10 markdown"},[e("router-view")],1)])])]),t._v(" "),t._m(2)])},staticRenderFns:[function(){var t=this.$createElement,a=this._self._c||t;return a("div",{staticClass:"navbar-brand"},[a("a",{staticClass:"navbar-item",attrs:{href:""}},[a("span",{staticClass:"logo"},[this._v("\n                  Frew"),a("i",{staticClass:"fas fa-angle-double-up"}),this._v("rd\n              ")])]),this._v(" "),a("div",{staticClass:"navbar-burger burger",attrs:{"data-target":"navbarLanguage"}},[a("span"),this._v(" "),a("span"),this._v(" "),a("span")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("div",{staticClass:"navbar-end"},[a("div",{staticClass:"navbar-item"},[a("div",{staticClass:"field is-grouped"},[a("p",{staticClass:"control"},[a("a",{staticClass:"bd-tw-button button",attrs:{href:"https://github.com/toanngosy/"}},[a("span",{staticClass:"icon"},[a("i",{staticClass:"fab fa-github"})])])]),this._v(" "),a("p",{staticClass:"control"},[a("a",{staticClass:"bd-tw-button button",attrs:{href:"https://www.linkedin.com/in/sy-toan-ngo-491109bb/"}},[a("span",{staticClass:"icon"},[a("i",{staticClass:"fab fa-linkedin"})])])]),this._v(" "),a("p",{staticClass:"control"},[a("a",{staticClass:"bd-tw-button button",attrs:{href:"https://www.facebook.com/toanngosy/"}},[a("span",{staticClass:"icon"},[a("i",{staticClass:"fab fa-facebook"})])])])])])])},function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("footer",{staticClass:"footer"},[e("div",{staticClass:"container"},[e("div",{staticClass:"content has-text-centered"},[e("p",[e("strong",[t._v("Frew"),e("i",{staticClass:"fas fa-angle-double-up"}),t._v("rd")]),t._v(" by "),e("a",{attrs:{href:"https://www.facebook.com/toanngosy/",target:"_blank"}},[t._v("Gonaton")]),t._v(". Mã nguồn bản quyền\n          "),e("a",{attrs:{href:"http://opensource.org/licenses/mit-license.php"}},[t._v("MIT")]),t._v(".\n        ")]),t._v(" "),e("p",[e("a",{staticClass:"icon",attrs:{href:"https://github.com/toanngosy/"}},[e("i",{staticClass:"fab fa-github"})])])])])])}]};var s=e("VU/8")(i,n,!1,function(t){e("b5jy")},"data-v-f0800906",null);a.default=s.exports},"5V1Y":function(t,a,e){"use strict";Object.defineProperty(a,"__esModule",{value:!0});var i={render:function(){this.$createElement;this._self._c;return this._m(0)},staticRenderFns:[function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("section",[e("h1",[t._v("Introduction")]),t._v(" "),e("p",[t._v("À côté des développements de Computer Vision, Natural Language Processing,\nReinforcement Learning a également obtenu des résultats surprenants\nces dernières années et reçu beaucoup d'attention.\nCependant, les tutoriels trouvés en ligne s’accélèrent\nsouvent jusqu’à la section de codage sans entrer profondément dans les mathématiques.\nCe blog est basé sur la lecture du professeur Sergey Levine de l’UC Berkeley\net le livre Introduction to Reinforcement Learning du Professeur Sutton et Barto,\nVous pouvez trouver le lien pour la lecture\n"),e("a",{attrs:{href:"http://rail.eecs.berkeley.edu/deeprlcourse/"}},[t._v("ici")]),t._v("\net le livre "),e("a",{attrs:{href:"http://incompleteideas.net/book/bookdraft2017nov5.pdf"}},[t._v("ici")]),t._v(".")]),t._v(" "),e("p",[t._v("This blog will also introduce RL libraries and how to use them.")]),t._v(" "),e("h1",[t._v("About Frew"),e("i",{staticClass:"fas fa-angle-double-up"}),t._v("rd")]),t._v(" "),e("p",[t._v("In RL, reward is important, actions taken by the AI are to\noptimize the total reward in long-term and constructing a reward function so that it's reasonable\nis one of the essential conditions for RL algorithms to work. In reality,\nreward are also important, which is the goal we aim for in life.\nReward function - \\(f(reward)\\) - Frew"),e("i",{staticClass:"fas fa-angle-double-up"}),t._v("rd is from here.\n"),e("br"),t._v(" "),e("br"),t._v(" "),e("br"),t._v(" "),e("br")]),t._v(" "),e("p",{staticStyle:{"font-size":"200%"},attrs:{align:"center"}},[t._v("In this Freeworld, what is your Frew"),e("i",{staticClass:"fas fa-angle-double-up"}),t._v("rd?")])])}]},n=e("VU/8")(null,i,!1,null,null,null);a.default=n.exports},"7ZQQ":function(t,a,e){t.exports=e("JGCX")},"9fpD":function(t,a,e){"use strict";var i={render:function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("aside",{staticClass:"list-group"},[e("ul",[e("li",[e("router-link",{staticClass:"list-group-item",attrs:{to:"/",exact:""}},[t._v("Lời mở đầu")])],1)]),t._v(" "),e("div",{staticClass:"menu"},[e("p",{staticClass:"menu-label"},[t._v("RL cơ bản")]),t._v(" "),e("ul",{staticClass:"menu-list"},[e("li",[e("router-link",{attrs:{to:"/vi/theorical/1",exact:""}},[t._v("DDPG")])],1),t._v(" "),e("li",[e("router-link",{attrs:{to:"/en/components/layout",exact:""}},[t._v("TRPO/PPO")])],1),t._v(" "),e("li",[e("router-link",{attrs:{to:"/en/components/button",exact:""}},[t._v("SAC")])],1),t._v(" "),e("li",[e("router-link",{attrs:{to:"/en/components/icon",exact:""}},[t._v("Inference")])],1)]),t._v(" "),e("p",{staticClass:"menu-label"},[t._v("RL luyện tập")]),t._v(" "),e("ul",{staticClass:"menu-list"},[e("li",[e("router-link",{attrs:{to:"/en/components/affix",exact:""}},[t._v("OpenAI Gym")])],1),t._v(" "),e("li",[e("router-link",{attrs:{to:"/en/components/scrollto",exact:""}},[t._v("Q-learning")])],1),t._v(" "),e("li",[e("router-link",{attrs:{to:"/en/components/breadcrumb",exact:""}},[t._v("DDPG")])],1),t._v(" "),e("li",[e("router-link",{attrs:{to:"/en/components/breadcrumb",exact:""}},[t._v("TRPO/PPO")])],1),t._v(" "),e("li",[e("router-link",{attrs:{to:"/en/components/breadcrumb",exact:""}},[t._v("SAC")])],1)]),t._v(" "),e("p",{staticClass:"menu-label"},[t._v("Thư viện RL")]),t._v(" "),e("ul",{staticClass:"menu-list"},[e("li",[e("router-link",{attrs:{to:"/en/components/alert",exact:""}},[t._v("OpenAI baselines")])],1),t._v(" "),e("li",[e("router-link",{attrs:{to:"/en/components/aside",exact:""}},[t._v("Stable baselines")])],1),t._v(" "),e("li",[e("router-link",{attrs:{to:"/en/components/collapse",exact:""}},[t._v("Ray/RLLib")])],1),t._v(" "),e("li",[e("router-link",{attrs:{to:"/en/components/modal",exact:""}},[t._v("Unity ML-Agents")])],1)]),t._v(" "),e("p",{staticClass:"menu-label"},[t._v("Ứng dụng")]),t._v(" "),e("ul",{staticClass:"menu-list"},[e("li",[e("router-link",{attrs:{to:"/en/components/form",exact:""}},[t._v("Frew"),e("i",{staticClass:"fas fa-angle-double-up"}),t._v("rd Env")])],1),t._v(" "),e("li",[e("router-link",{attrs:{to:"/en/components/form2",exact:""}},[t._v("Frew"),e("i",{staticClass:"fas fa-angle-double-up"}),t._v("rd Exp")])],1)])])])},staticRenderFns:[]};var n=e("VU/8")(null,i,!1,function(t){e("olKP")},"data-v-2fee8012",null);a.a=n.exports},BVxU:function(t,a,e){t.exports=e("tz1K")},C05T:function(t,a,e){"use strict";var i={render:function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("aside",{staticClass:"list-group"},[e("ul",[e("li",[e("router-link",{staticClass:"list-group-item",attrs:{to:"/",exact:""}},[t._v("Introduction")])],1)]),t._v(" "),e("div",{staticClass:"menu"},[e("p",{staticClass:"menu-label"},[t._v("Theorical RL")]),t._v(" "),e("ul",{staticClass:"menu-list"},[e("li",[e("router-link",{attrs:{to:"/theorical/1",exact:""}},[t._v("DDPG")])],1),t._v(" "),e("li",[e("router-link",{attrs:{to:"/components/layout",exact:""}},[t._v("TRPO/PPO")])],1),t._v(" "),e("li",[e("router-link",{attrs:{to:"/components/button",exact:""}},[t._v("SAC")])],1),t._v(" "),e("li",[e("router-link",{attrs:{to:"/components/icon",exact:""}},[t._v("Inference")])],1)]),t._v(" "),e("p",{staticClass:"menu-label"},[t._v("Practical RL")]),t._v(" "),e("ul",{staticClass:"menu-list"},[e("li",[e("router-link",{attrs:{to:"/en/components/affix",exact:""}},[t._v("OpenAI Gym")])],1),t._v(" "),e("li",[e("router-link",{attrs:{to:"/en/components/scrollto",exact:""}},[t._v("Q-learning")])],1),t._v(" "),e("li",[e("router-link",{attrs:{to:"/en/components/breadcrumb",exact:""}},[t._v("DDPG")])],1),t._v(" "),e("li",[e("router-link",{attrs:{to:"/en/components/breadcrumb",exact:""}},[t._v("TRPO/PPO")])],1),t._v(" "),e("li",[e("router-link",{attrs:{to:"/en/components/breadcrumb",exact:""}},[t._v("SAC")])],1)]),t._v(" "),e("p",{staticClass:"menu-label"},[t._v("Library Review")]),t._v(" "),e("ul",{staticClass:"menu-list"},[e("li",[e("router-link",{attrs:{to:"/en/components/alert",exact:""}},[t._v("OpenAI baselines")])],1),t._v(" "),e("li",[e("router-link",{attrs:{to:"/en/components/aside",exact:""}},[t._v("Stable baselines")])],1),t._v(" "),e("li",[e("router-link",{attrs:{to:"/en/components/collapse",exact:""}},[t._v("Ray/RLLib")])],1),t._v(" "),e("li",[e("router-link",{attrs:{to:"/en/components/modal",exact:""}},[t._v("Unity ML-Agents")])],1)]),t._v(" "),e("p",{staticClass:"menu-label"},[t._v("Application")]),t._v(" "),e("ul",{staticClass:"menu-list"},[e("li",[e("router-link",{attrs:{to:"/en/components/form",exact:""}},[t._v("Frew"),e("i",{staticClass:"fas fa-angle-double-up"}),t._v("rd Env")])],1),t._v(" "),e("li",[e("router-link",{attrs:{to:"/en/components/form2",exact:""}},[t._v("Frew"),e("i",{staticClass:"fas fa-angle-double-up"}),t._v("rd Exp")])],1)])])])},staticRenderFns:[]};var n=e("VU/8")(null,i,!1,function(t){e("FXma")},"data-v-7817a196",null);a.a=n.exports},DC16:function(t,a,e){"use strict";Object.defineProperty(a,"__esModule",{value:!0});var i={components:{SideMenu:e("pukS").a}},n={render:function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("div",[e("nav",{staticClass:"navbar is-fixed-top is-light is-bold"},[t._m(0),t._v(" "),e("div",{staticClass:"navbar-menu",attrs:{id:"navbarLanguage"}},[e("div",{staticClass:"navbar-start"},[e("div",{staticClass:"navbar-item has-dropdown is-hoverable"},[e("a",{staticClass:"navbar-link"},[t._v("\n                      Langue\n                  ")]),t._v(" "),e("div",{staticClass:"navbar-dropdown is-boxed"},[e("a",{staticClass:"navbar-item"},[e("router-link",{attrs:{to:"/"}},[e("span",[t._v("Anglais")])])],1),t._v(" "),e("a",{staticClass:"navbar-item"},[e("router-link",{attrs:{to:"/vi"}},[e("span",[t._v("Vietnamien")])])],1),t._v(" "),e("a",{staticClass:"navbar-item"},[e("router-link",{attrs:{to:"/fr"}},[e("span",[t._v("Français")])])],1)])])]),t._v(" "),t._m(1)])]),t._v(" "),t._m(2),t._v(" "),e("section",{staticClass:"section"},[e("div",{staticClass:"container"},[e("div",{staticClass:"columns"},[e("div",{staticClass:"column is-2 side-nav"},[e("side-menu")],1),t._v(" "),e("div",{staticClass:"column is-10 markdown"},[e("router-view")],1)])])]),t._v(" "),t._m(3)])},staticRenderFns:[function(){var t=this.$createElement,a=this._self._c||t;return a("div",{staticClass:"navbar-brand"},[a("a",{staticClass:"navbar-item",attrs:{href:""}},[a("span",{staticClass:"logo"},[this._v("\n                  Frew"),a("i",{staticClass:"fas fa-angle-double-up"}),this._v("rd\n              ")])]),this._v(" "),a("div",{staticClass:"navbar-burger burger",attrs:{"data-target":"navbarLanguage"}},[a("span"),this._v(" "),a("span"),this._v(" "),a("span")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("div",{staticClass:"navbar-end"},[a("div",{staticClass:"navbar-item"},[a("div",{staticClass:"field is-grouped"},[a("p",{staticClass:"control"},[a("a",{staticClass:"bd-tw-button button",attrs:{href:"https://github.com/toanngosy/"}},[a("span",{staticClass:"icon"},[a("i",{staticClass:"fab fa-github"})])])]),this._v(" "),a("p",{staticClass:"control"},[a("a",{staticClass:"bd-tw-button button",attrs:{href:"https://www.linkedin.com/in/sy-toan-ngo-491109bb/"}},[a("span",{staticClass:"icon"},[a("i",{staticClass:"fab fa-linkedin"})])])]),this._v(" "),a("p",{staticClass:"control"},[a("a",{staticClass:"bd-tw-button button",attrs:{href:"https://www.facebook.com/toanngosy/"}},[a("span",{staticClass:"icon"},[a("i",{staticClass:"fab fa-facebook"})])])])])])])},function(){var t=this.$createElement,a=this._self._c||t;return a("section",{staticClass:"hero is-dark is-medium"},[a("div",{staticClass:"hero-body"},[a("div",{staticClass:"container"},[a("span",{staticClass:"randomimage"},[a("i",{staticClass:"fas fa-brain"})]),this._v(" "),a("span",{staticClass:"welcome"},[this._v("Bienvenue à Frew"),a("i",{staticClass:"fas fa-angle-double-up"}),this._v("rd")])])])])},function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("footer",{staticClass:"footer"},[e("div",{staticClass:"container"},[e("div",{staticClass:"content has-text-centered"},[e("p",[e("strong",[t._v("Frew"),e("i",{staticClass:"fas fa-angle-double-up"}),t._v("rd")]),t._v(" par "),e("a",{attrs:{href:"https://www.facebook.com/toanngosy/",target:"_blank"}},[t._v("Gonaton")]),t._v(". Le code source est sous licence\n          "),e("a",{attrs:{href:"http://opensource.org/licenses/mit-license.php"}},[t._v("MIT")]),t._v(".\n        ")]),t._v(" "),e("p",[e("a",{staticClass:"icon",attrs:{href:"https://github.com/toanngosy/"}},[e("i",{staticClass:"fab fa-github"})])])])])])}]};var s=e("VU/8")(i,n,!1,function(t){e("2KgI")},"data-v-6798b13e",null);a.default=s.exports},DPxC:function(t,a,e){t.exports=e("5V1Y")},E6CH:function(t,a){},E8C0:function(t,a,e){"use strict";Object.defineProperty(a,"__esModule",{value:!0});var i={components:{SideMenu:e("C05T").a}},n={render:function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("div",[e("nav",{staticClass:"navbar is-fixed-top is-light is-bold"},[t._m(0),t._v(" "),e("div",{staticClass:"navbar-menu",attrs:{id:"navbarLanguage"}},[e("div",{staticClass:"navbar-start"},[e("div",{staticClass:"navbar-item has-dropdown is-hoverable"},[e("a",{staticClass:"navbar-link"},[t._v("\n                      Language\n                  ")]),t._v(" "),e("div",{staticClass:"navbar-dropdown is-boxed"},[e("a",{staticClass:"navbar-item"},[e("router-link",{attrs:{to:"/"}},[e("span",[t._v("English")])])],1),t._v(" "),e("a",{staticClass:"navbar-item"},[e("router-link",{attrs:{to:"/vi"}},[e("span",[t._v("Vietnamese")])])],1),t._v(" "),e("a",{staticClass:"navbar-item"},[e("router-link",{attrs:{to:"/fr"}},[e("span",[t._v("French")])])],1)])])]),t._v(" "),t._m(1)])]),t._v(" "),t._m(2),t._v(" "),e("section",{staticClass:"section"},[e("div",{staticClass:"container"},[e("div",{staticClass:"columns"},[e("div",{staticClass:"column is-2 side-nav"},[e("side-menu")],1),t._v(" "),e("div",{staticClass:"column is-10 markdown"},[e("router-view")],1)])])]),t._v(" "),t._m(3)])},staticRenderFns:[function(){var t=this.$createElement,a=this._self._c||t;return a("div",{staticClass:"navbar-brand"},[a("a",{staticClass:"navbar-item",attrs:{href:""}},[a("span",{staticClass:"logo"},[this._v("\n                  Frew"),a("i",{staticClass:"fas fa-angle-double-up"}),this._v("rd\n              ")])]),this._v(" "),a("div",{staticClass:"navbar-burger burger",attrs:{"data-target":"navbarLanguage"}},[a("span"),this._v(" "),a("span"),this._v(" "),a("span")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("div",{staticClass:"navbar-end"},[a("div",{staticClass:"navbar-item"},[a("div",{staticClass:"field is-grouped"},[a("p",{staticClass:"control"},[a("a",{staticClass:"bd-tw-button button",attrs:{href:"https://github.com/toanngosy/"}},[a("span",{staticClass:"icon"},[a("i",{staticClass:"fab fa-github"})])])]),this._v(" "),a("p",{staticClass:"control"},[a("a",{staticClass:"bd-tw-button button",attrs:{href:"https://www.linkedin.com/in/sy-toan-ngo-491109bb/"}},[a("span",{staticClass:"icon"},[a("i",{staticClass:"fab fa-linkedin"})])])]),this._v(" "),a("p",{staticClass:"control"},[a("a",{staticClass:"bd-tw-button button",attrs:{href:"https://www.facebook.com/toanngosy/"}},[a("span",{staticClass:"icon"},[a("i",{staticClass:"fab fa-facebook"})])])])])])])},function(){var t=this.$createElement,a=this._self._c||t;return a("section",{staticClass:"hero is-dark is-medium"},[a("div",{staticClass:"hero-body"},[a("div",{staticClass:"container"},[a("span",{staticClass:"randomimage"},[a("i",{staticClass:"fas fa-brain"})]),this._v(" "),a("span",{staticClass:"welcome"},[this._v("Welcome to Frew"),a("i",{staticClass:"fas fa-angle-double-up"}),this._v("rd")])])])])},function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("footer",{staticClass:"footer"},[e("div",{staticClass:"container"},[e("div",{staticClass:"content has-text-centered"},[e("p",[e("strong",[t._v("Frew"),e("i",{staticClass:"fas fa-angle-double-up"}),t._v("rd")]),t._v(" by "),e("a",{attrs:{href:"https://www.facebook.com/toanngosy/",target:"_blank"}},[t._v("Gonaton")]),t._v(". The source code is licensed\n          "),e("a",{attrs:{href:"http://opensource.org/licenses/mit-license.php"}},[t._v("MIT")]),t._v(".\n        ")]),t._v(" "),e("p",[e("a",{staticClass:"icon",attrs:{href:"https://github.com/toanngosy/"}},[e("i",{staticClass:"fab fa-github"})])])])])])}]};var s=e("VU/8")(i,n,!1,function(t){e("JUWQ")},"data-v-4bcf43b8",null);a.default=s.exports},FXma:function(t,a){},JGCX:function(t,a,e){"use strict";Object.defineProperty(a,"__esModule",{value:!0});var i={render:function(){this.$createElement;this._self._c;return this._m(0)},staticRenderFns:[function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("section",[e("h1",[t._v("Introduction")]),t._v(" "),e("p",[t._v("Besides the development of Computer Vision, Natural Language Processing,\nReinforcement Learning also achieved suprising results in recent years\nand received much attention.\nHowever, tutorials found online often speed up to coding section without\ngoing deeply into math.\nThis blog is based on a lecture given by Professor Sergey Levine of UC Berkeley\nand the Introduction to Reinforcement Learning book by Professor Sutton and Barto,\nYou can find the link for the lecture\n"),e("a",{attrs:{href:"http://rail.eecs.berkeley.edu/deeprlcourse/"}},[t._v("here")]),t._v("\nand the book "),e("a",{attrs:{href:"http://incompleteideas.net/book/bookdraft2017nov5.pdf"}},[t._v("here")]),t._v(".")]),t._v(" "),e("p",[t._v("This blog will also introduce RL libraries and how to use them.")]),t._v(" "),e("h1",[t._v("About Frew"),e("i",{staticClass:"fas fa-angle-double-up"}),t._v("rd")]),t._v(" "),e("p",[t._v("In RL, reward is important, actions taken by the AI are to\noptimize the total reward in long-term and constructing a reward function so that it's reasonable\nis one of the essential conditions for RL algorithms to work. In reality,\nreward are also important, which is the goal we aim for in life.\nReward function - \\(f(reward)\\) - Frew"),e("i",{staticClass:"fas fa-angle-double-up"}),t._v("rd is from here.\n"),e("br"),t._v(" "),e("br"),t._v(" "),e("br"),t._v(" "),e("br")]),t._v(" "),e("p",{staticStyle:{"font-size":"200%"},attrs:{align:"center"}},[t._v("In this Freeworld, what is your Frew"),e("i",{staticClass:"fas fa-angle-double-up"}),t._v("rd?")])])}]},n=e("VU/8")(null,i,!1,null,null,null);a.default=n.exports},JUWQ:function(t,a){},KAWA:function(t,a){},Kjo4:function(t,a,e){"use strict";Object.defineProperty(a,"__esModule",{value:!0});var i={render:function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("section",[e("p",[e("vue-mathjax")],1),t._v(" "),e("h1",[t._v("I - Reinforcement Learning - from Policy Gradient to Deep Deterministic Policy Gradient")]),t._v(" "),t._m(0),t._v(" "),t._m(1),t._v(" "),e("p",[t._v("Voici les définitions des termes communs en RL:")]),t._v(" "),t._m(2),t._v(" "),t._m(3),t._v(" "),e("br"),t._v(" "),e("p",[t._v("Dans un état $s$, l'agent interagit avec l'environnement par l'action $a$,\nconduisant à un nouvel état $ s_{t+1}$ et recevez une récompense $r_{t+1}$.\nLa boucle se répète ainsi jusqu'à ce que l'état final atteigne $s_T$.")]),t._v(" "),e("p",[t._v("Dans la section ci-dessous, je vais utiliser les termes Anglais à suivre au lieu de traduire en Français.")]),t._v(" "),e("h1",[t._v("1 - Example")]),t._v(" "),t._m(4),t._v(" "),t._m(5),t._v(" "),e("br"),t._v(" "),t._m(6),t._v(" "),e("h1",[t._v("2 - Policy Gradient")]),t._v(" "),e("p",[t._v("Pour un exemple vivant, nous examinons un problème de jeu simple, le jeu Hare Egg.")]),t._v(" "),t._m(7),t._v(" "),e("br"),t._v(" "),e("p",[t._v("Soit $\\pi_\\theta(a|s) = f(s, \\theta)$ est le policy de l'agent, c'est une distribution de probabilité d'action $a$ à l'état $s$.")]),t._v(" "),t._m(8),t._v(" "),e("p",[t._v("Soit $\\tau = s_1, a_1, s_2, a_2,..., s_T, a_T$ est la séquence de l'état $s_1$  à l'état $s_T$. La probabilité de $\\tau$ est susceptible de se produire:")]),t._v(" "),e("p",[t._v("[\n\\begin{eqnarray}\np_\\theta(\\tau) &=& p_\\theta(s_1, a_1, s_2, a_2,...s_T, a_T) \\\\\n&=& p(s_1)\\pi_\\theta(a_1|s_1)p(s_2|s_1, a_1)\\pi_\\theta(a_2|s_2)...p(s_{T}|s_{T-1},a_{T-1})\\pi_\\theta(a_T|s_T) \\\\\n&=& p(s_1)\\Pi_{t=1}^{t=T}\\pi_\\theta(a_t|s_t)p(s_{t+1}|s_t, a_t) \\\\\n\\end{eqnarray}\n]")]),t._v(" "),e("p",[t._v("Nous verrons que la distribution de probabilité de l'état $p(s_{t+1}|a_t, s_t)$ sera éliminée plus tard.")]),t._v(" "),t._m(9),t._v(" "),e("p",[t._v("$$\n\\begin{eqnarray}\n\\theta^* &=& \\arg\\max_\\theta E_{\\tau\\sim p_\\theta(\\tau)}\\big[r(\\tau)\\big] \\\\\n&=& \\arg\\max_\\theta E_{\\tau\\sim p_\\theta(\\tau)}\\bigg[\\sum_t r(a_t, s_t)\\bigg]\n\\end{eqnarray}\n$$")]),t._v(" "),t._m(10),t._v(" "),e("p",[t._v("$$\n\\begin{eqnarray}\nJ(\\theta) &=& E_{\\tau\\sim p_\\theta(\\tau)}\\bigg[\\sum_t r(a_t, s_t)\\bigg] \\\\\n&=& \\frac{1}{N} \\sum_i\\sum_t r(a_t, s_t)\n\\end{eqnarray}\n$$")]),t._v(" "),t._m(11),t._v(" "),e("p",[t._v("$$\n\\begin{eqnarray}\nJ(\\theta) &=& E_{\\tau\\sim p_\\theta(\\tau)}\\bigg[\\sum_t r(a_t, s_t)\\bigg] \\\\\n&=& \\int p_\\theta(\\tau) r(\\tau) dr\n\\end{eqnarray}\n$$")]),t._v(" "),e("p",[t._v("Continuant à examiner le gradient de la fonction objectif:")]),t._v(" "),e("p",[t._v("$$\n\\begin{eqnarray}\n\\nabla_\\theta J(\\theta) &=& E_{\\tau\\sim p_\\theta(\\tau)}\\bigg[\\sum_t r(a_t, s_t)\\bigg] \\\\\n&=& \\int \\nabla_\\theta p_\\theta(\\tau) r(\\tau) dr\n\\end{eqnarray}\n$$\nMais nous avons aussi:\n$$\n\\begin{eqnarray}\n\\nabla_\\theta p_\\theta(\\tau) &=&  p_\\theta(\\tau) \\frac{\\nabla_\\theta p_\\theta(\\tau)} {p_\\theta(\\tau)} \\\\\n&=& p_\\theta(\\tau)\\nabla_\\theta \\log p_\\theta(\\tau)\n\\end{eqnarray}\n$$\n** Notez ** que cette trick est utilisée souvant, donc:\n$$\n\\begin{eqnarray}\n\\nabla_\\theta J(\\theta) &=& \\int p_\\theta(\\tau) \\nabla_\\theta \\log p_\\theta(\\tau) r(\\tau) dr \\\\\n&=& E_{\\tau\\sim p_\\theta(\\tau)}\\bigg[\\nabla_\\theta \\log p_\\theta(\\tau) r(\\tau)\\bigg]\n\\end{eqnarray}\n$$")]),t._v(" "),e("p",[t._v("Examinons de plus sur $\\log p_\\theta(\\tau)$, comme nous l'avons vu plus haut  $p_\\theta(\\tau) = p(s_1)\\Pi_{t=1}^{t=T}\\pi_\\theta(a_t|s_t)p(s_{t+1}|s_t, a_t)$, nous avons:")]),t._v(" "),e("p",[t._v("$$\n\\begin{eqnarray}\n\\log p_\\theta(\\tau) = \\log p(s_1) + \\sum_{t=1}^{t=T}\\log \\pi_\\theta(a_t|s_t) + \\sum_{t=1}^{t=T}\\log p(s_{t+1}|s_t, a_t)\n\\end{eqnarray}\n$$")]),t._v(" "),e("p",[t._v("Finalement:\n$$\n\\begin{eqnarray}\n\\nabla_\\theta \\log p_\\theta(\\tau) = \\sum_{t=1}^{t=T}\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\n\\end{eqnarray}\n$$")]),t._v(" "),t._m(12),t._v(" "),e("p",[t._v("Le gradient de la fonction objectif devient maintenant:\n$$\n\\begin{eqnarray}\n\\nabla_\\theta J(\\theta) &=&  E_{\\tau\\sim p_\\theta(\\tau)}\\bigg[\\nabla_\\theta \\log p_\\theta(\\tau) r(\\tau)\\bigg] \\\\\n&=& E_{\\tau\\sim p_\\theta(\\tau)}\\bigg[\\sum_{t=1}^{t=T}\\nabla_\\theta \\log\\pi_\\theta(a_t|s_t)\\sum_{t=1}^{t=T} r(a_t, s_t)\\bigg]\n\\end{eqnarray}\n$$")]),t._v(" "),e("p",[t._v("De même, après avoir passé $N$ épisodes , l'expectation de ce gradient est la suivante:\n$$\n\\begin{eqnarray}\n\\nabla_\\theta J(\\theta) &=& \\frac{1}{N}\\sum_{i=1}^{N}\\bigg(\\sum_{t=1}^{t=T}\\nabla_\\theta \\log \\pi_\\theta(a_{i,t}|s_{i,t})\\bigg)\\bigg(\\sum_{t=1}^{t=T} r(a_{i,t}, s_{i,t})\\bigg)\n\\end{eqnarray}\n$$")]),t._v(" "),e("p",[t._v("Enfin, mettre à jour $\\theta$ en utilisant un gradient ascendant:\n$$\n\\begin{eqnarray}\n\\theta \\leftarrow \\theta + \\nabla_\\theta J(\\theta)\n\\end{eqnarray}\n$$")]),t._v(" "),e("h1",[t._v("3 - L'algorithme REINFORCE")]),t._v(" "),e("p",[t._v("Résumez tous les résultats ci-dessus, nous avons l'algorithme REINFORCE comme ci-dessous:")]),t._v(" "),t._m(13),t._v(" "),e("p",[t._v("Maintenant, on se pauser pour regarder de plus sur le gradient de la fonction objectif. Écrivez sous une forme simple, nous avons:")]),t._v(" "),t._m(14),t._v(" "),e("h1",[t._v("4 - Quelques nouvelles définitions")]),t._v(" "),t._m(15),t._v(" "),e("p",[t._v("$G_t=\\sum^{\\infty}_{k=0}\\gamma^kR_{k+t+1}$: la somme de toutes les récompenses sera reçue de l'état $s_t$ à l'avenir, avec le facteur de remise $\\gamma$: $0 < \\gamma < 1$. Plus loin dans le futur, la reward sera moins prise en compte, l'agent concerne les rewards plus approches que des rewards lointaines.")]),t._v(" "),e("h2",[t._v("4.1 - Les équations de Bellman")]),t._v(" "),e("p",[t._v("De la formule ci-dessus, nous avons:")]),t._v(" "),e("p",[t._v("$$\n\\begin{eqnarray}\nV^\\pi(s_t) &=& E_\\pi\\bigg[G_t|S=s_t\\bigg] \\\\\n&=& E_\\pi\\bigg[\\sum^{\\infty}_{k=0}\\gamma^kR_{k+t+1}|S=s_t\\bigg] \\\\\n\\end{eqnarray}\n$$")]),t._v(" "),e("p",[t._v("Prenez la récompense $R_{t+1}$ reçue lorsque il passe de l'état $s_t$ à $s_{t+1}$ en dehors de $\\sum$, nous obtenons:")]),t._v(" "),e("p",[t._v("$$\n\\begin{eqnarray}\nE_\\pi\\bigg[R_{t+1} + \\gamma\\sum^{\\infty}_{k=0}\\gamma^kR_{k+t+2}|S=s_t\\bigg] &=& E_\\pi[R_{t+1}|S=s_t] + \\gamma E_\\pi\\bigg[\\sum^{\\infty}_{k=0}\\gamma^kR_{k+t+2}|S=s_t\\bigg]\n\\end{eqnarray}\n$$")]),t._v(" "),e("p",[t._v("Développez les 2 éxpectations de l'équation ci-dessus, nous avons:")]),t._v(" "),e("p",[t._v("$$\n\\begin{eqnarray}\nE_\\pi\\bigg[R_{t+1}|S=s_t\\bigg]=\\sum_a \\pi(s_t,a) \\sum_{s_{t+1}} p(s_{t+1}|s_t, a)R(s_{t+1}|s_t, a)\n\\end{eqnarray}\n$$\nMais:")]),t._v(" "),e("p",[t._v("$$\n\\begin{eqnarray}\n\\gamma E_\\pi\\bigg[\\sum^{\\infty}_{k=0}\\gamma^kR_{k+t+2}|S=s_t\\bigg] = \\sum_a \\pi(s_t,a) \\sum_{s_{t+1}} p(s_{t+1}|s_t, a)\\gamma E_\\pi\\bigg[\\sum^\\infty_{k=0} \\gamma^k R_{t+k+2} | S = s_{t+1}\\bigg]\n\\end{eqnarray}\n$$\nNous avons:\n$$\n\\begin{eqnarray}\nV^\\pi(s_t) = \\sum_a \\pi(s_t,a) \\sum_{s_{t+1}} p(s_{t+1}|s_t, a)\\Bigg[R(s_{t+1}|s_t, a) + \\gamma E_\\pi\\bigg[\\sum^\\infty_{k=0} \\gamma^k R_{t+k+2} | S = s_{t+1} \\bigg]\\Bigg]\n\\end{eqnarray}\n$$\nRemarquerez que:\n$$\n\\begin{eqnarray}\nE_\\pi\\bigg[\\sum^\\infty_{k=0} \\gamma^k R_{t+k+2} | S = s_{t+1}\\bigg] = V^\\pi(s_{t+1})\n\\end{eqnarray}\n$$")]),t._v(" "),e("p",[t._v("Enfin nous avons:\n$$\n\\begin{eqnarray}\nV^\\pi(s_t) = \\sum_a \\pi(s_t,a) \\sum_{s_{t+1}} p(s_{t+1}|s_t, a)\\bigg[R(s_{t+1}|s_t, a) + \\gamma  V^\\pi(s_{t+1})\\bigg]\n\\end{eqnarray}\n$$")]),t._v(" "),e("p",[t._v("Faire la même chose avec $Q^\\pi(s_t, a_t)$:\n$$\n\\begin{eqnarray}\nQ^\\pi(s_t, a_t) = \\sum_{s_{t+1}} p(s_{t+1}|s_t, a)\\bigg[R(s_{t+1}|s_t, a) + \\gamma \\sum_{a_{t+1}} \\pi(s_{t+1}, a_{t+1}) Q^\\pi (s_{t+1}, a_{t+1}) \\bigg]\n\\end{eqnarray}\n$$\nCombinez avec la relation entre $V^\\pi$ and $Q^\\pi$ ci-dessus, nous avons:\n$$\n\\begin{eqnarray}\n\\sum_{a_{t+1}} \\pi(s_{t+1}, a_{t+1}) Q^\\pi (s_{t+1}, a_{t+1}) = V^\\pi(s_{t+1})\n\\end{eqnarray}\n$$\nDonc:\n$$\n\\begin{eqnarray}\nQ^\\pi(s_t, a_t) = \\sum_{s_{t+1}} p(s_{t+1}|s_t, a_t)\\bigg[R(s_{t+1}|s_t, a_t) + \\gamma  V^\\pi(s_{t+1}) \\bigg]\n\\end{eqnarray}\n$$")]),t._v(" "),e("p",[t._v("Par conséquent, si nous connaissons la valeur à l'état $ s_ {t + 1} $, nous pouvons facilement calculer la valeur à l'état $ s_t $. En résumé, nous avons 2 formules ci-dessous:\nTout ce qui précède montre que nous pouvons représenter la valeur de $Q^\\pi$ et $V^\\pi$ à l'état $s_t$ avec l'état $s_{t+1}$. Par conséquent, si nous connaissons la valeur à l'état $s_{t+1}$, nous pouvons facilement calculer la valeur à l'état $s_t$. En résumé, nous avons 2 formules ci-dessous:")]),t._v(" "),e("p",[t._v("$$\n\\begin{eqnarray}\nV^\\pi(s_t) &=& \\sum_a \\pi(s_t,a) \\sum_{s_{t+1}} p(s_{t+1}|s_t, a)\\bigg[R(s_{t+1}|s_t, a) + \\gamma  V^\\pi(s_{t+1})\\bigg] \\\\\nQ^\\pi(s_t, a_t) &=& \\sum_{s_{t+1}} p(s_{t+1}|s_t, a_t)\\bigg[R(s_{t+1}|s_t, a_t) + \\gamma  V^\\pi(s_{t+1}) \\bigg]\n\\end{eqnarray}\n$$")]),t._v(" "),e("p",[t._v("Revenir au gradient de la fonction objectif, nous avons maintenant:\n$$\n\\begin{eqnarray}\n\\nabla_\\theta J(\\theta) =  E_{\\tau\\sim p_\\theta(\\tau), a\\sim\\pi_\\theta}\\bigg[\\nabla_\\theta \\log\\pi_\\theta(a|s)Q^\\pi(s,a)\\bigg]\n\\end{eqnarray}\n$$")]),t._v(" "),e("h1",[t._v("5 - Advantage")]),t._v(" "),e("p",[t._v("$$\n\\begin{eqnarray}\n\\nabla_\\theta J(\\theta) =  E_{\\tau\\sim p_\\theta(\\tau), a\\sim\\pi_\\theta}\\bigg[\\nabla_\\theta \\log\\pi_\\theta(a|s)Q^\\pi(s,a)\\bigg]\n\\end{eqnarray}\n$$")]),t._v(" "),t._m(16),t._v(" "),e("p",[t._v("Comme dans la partie 4, nous avons $V^\\pi(s)$ est l’attente d’une reward accumulée à l’état $s$, quelle que soit la décision prise par l’agent à l’état $s$, nous nous attendons à une reward accumulée $V^\\pi(s)$ de là à la fin.\nPar conséquent, une action $a_m$ est mauvaise si $Q^\\pi(s,a_m)$ < $V^\\pi(s)$ et une action $a_n$ est bonne si $Q^\\pi(s,a_n)$ > $V^\\pi(s)$. À partir de\nlà, nous avons un ligne de base pour comparer $Q^\\pi(s,a)$ qui est $V^\\pi(s)$. Le gradient de la fonction objectif peut maintenant être écrit:")]),t._v(" "),e("p",[t._v("$$\n\\begin{eqnarray}\n\\nabla_\\theta J(\\theta) =  E_{\\tau\\sim p_\\theta(\\tau), a\\sim\\pi_\\theta}\\bigg[\\nabla_\\theta \\log\\pi_\\theta(a|s)\\Big(Q^\\pi(s,a)-V^\\pi(s)\\Big)\\bigg]\n\\end{eqnarray}\n$$")]),t._v(" "),t._m(17),t._v(" "),e("h1",[t._v("6 - Stochastic Actor-Critic")]),t._v(" "),t._m(18),t._v(" "),e("p",[t._v("Examinez le gradient de la fonction objectif que nous avons ci-dessus:\n$$\n\\begin{eqnarray}\n\\nabla_\\theta J(\\theta) =  E_{\\tau\\sim p_\\theta(\\tau), a\\sim\\pi_\\theta}\\bigg[\\nabla_\\theta \\log\\pi_\\theta(a|s)\\Big(Q^\\pi(s,a)-V^\\pi(s)\\Big)\\bigg]\n\\end{eqnarray}\n$$")]),t._v(" "),e("p",[t._v("À partir de l'équation de Bellman, nous avons la relation entre $Q^\\pi$ et $V^\\pi$, maintenant la fonction objectif devient:\n$$\n\\begin{eqnarray}\n\\nabla_\\theta J(\\theta) =  E_{\\tau\\sim p_\\theta(\\tau), a\\sim \\pi_\\theta}\\bigg[\\nabla_\\theta \\log\\pi_\\theta(a|s)\\Big(R + \\gamma V^\\pi(s_{t+1})- V^\\pi(s)\\Big)\\bigg]\n\\end{eqnarray}\n$$")]),t._v(" "),t._m(19),t._v(" "),t._m(20),t._v(" "),e("h1",[t._v("7 - L'algorithm Actor-Critic")]),t._v(" "),e("p",[t._v("À partir de l'algorithme REINFORCE, nous utilisons maintenant une fonction d'approximation supplémentaire pour la fonction de valeur $V_\\phi$, qui change un peu et nous avons:")]),t._v(" "),t._m(21),t._v(" "),e("h1",[t._v("8 - De Stochastic Actor-Critic à Q-Learning")]),t._v(" "),t._m(22),t._v(" "),t._m(23),t._v(" "),e("br"),t._v("\nTherefore, with a policy $\\pi$, we always can apply policy $\\pi'$ over it to have a new policy equal or better."),e("br"),t._v("\nWe now have the algorithm as follow:"),e("br"),t._v("\n1. Evaluate $A^\\pi(s,a)$ with different actions $a$ "),e("br"),t._v("\n2. Optimize $\\pi \\leftarrow \\pi'$\n"),t._m(24),t._v(" "),t._m(25),t._v(" "),e("p",[t._v("Now we actually do not need to care about policy anymore, and the second step can be written as:")]),t._v(" "),t._m(26),t._v(" "),e("p",[t._v("If we use an approximation function for $V_\\phi(s)$, we have the following algorithm:")]),t._v(" "),t._m(27),t._v(" "),e("p",[t._v("This algorithm is not good, in the first step we need to have reward $r(s, a)$ corresponding to different actions $a$, thus we need different simulations at a state $s$. To solve this, we could do the same analysis above with $Q(s,a)$ instead of $V(s)$.")]),t._v(" "),t._m(28),t._v(" "),t._m(29),t._v(" "),t._m(30),t._v(" "),t._m(31),t._v(" "),e("p",[t._v("To sum up, for the algorithm to be stable, and possibly converge, we need:")]),t._v(" "),t._m(32),t._v(" "),e("p",[t._v("The algorithm now:")]),t._v(" "),t._m(33),t._v(" "),e("h1",[t._v("9 - De Deep Q-Network à Deep Deterministic Policy Gradient")]),t._v(" "),e("p",[t._v("DQN algorithm is succeeded to approximate Q-value, but there is a limitation in step 3: we need to evaluate $Q_{\\phi'}$ with all different actions to choose the highest $Q$. With discrete action space such as games, when the set of actions is just up down left right buttons, the number of actions is finite and small, this is possible. However, in continuous action space, for example with action in a range from 0 to 1 we need a different approach.\nThe first thing we could think of is to discretize the action space into bins, for example from 0 to 1, we could divide it by 5 or 10 bins. Another way is that we sample actions with uniform distribution on the action space and choose the highest $Q(s,a)$ at state $s$.")]),t._v(" "),t._m(34),t._v(" "),e("p",[t._v("Now, if we add an approximation function $\\mu_\\theta(s) = \\arg max_a Q_\\phi(s,a)$, we will have to find $\\theta$ in which: $\\theta \\leftarrow  \\arg max_\\theta Q_\\phi(s,\\mu_\\theta(s))$. This optimization evaluates the change of $Q_\\phi$ w.r.t parameters $\\theta$. We could evaluate this change by the chain rule as follow:\n$\\frac{dQ_\\phi}{d\\theta} = \\frac{dQ_\\phi}{d\\mu} \\frac{d\\mu}{d\\theta}$.")]),t._v(" "),e("p",[t._v("Notice that, $\\mu_\\theta(s)$ is a Deterministic Policy, therefore this method is called Deep Deterministic Policy Gradient.")]),t._v(" "),e("p",[t._v("The DDPG algorithm is as follow:")]),t._v(" "),t._m(35),t._v(" "),t._m(36),t._v(" "),e("h1",[t._v("10 - Conclusion")]),t._v(" "),e("p",[t._v("To sum up, we went throught the Policy Gradient algorithm to DQN and DDPG.")]),t._v(" "),e("p",[t._v("Take away:")]),t._v(" "),t._m(37)])},staticRenderFns:[function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("p",[t._v("L’apprentissage par renforcement est le domaine associé à l'enseignement de machine "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("agent"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(" pour bien exécuter une tâche "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("task"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(" en interagissant avec l'environnement "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("environment"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(" et recevoir des récompenses "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("reward"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(".\nCette façon d’apprentissage est très similaire à la façon dont l’homme apprend en utilisant la mauvaise méthode de test. En prenant l'exemple d'un enfant en hiver, l'enfant aura tendance à se rapprocher du feu "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("car la récompense est chaleureuse"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(", mais également le feu est chaud, l'enfant aura tendance à éviter de toucher le feu "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("il le brûlera"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(".")])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("Dans l'exemple ci-dessus, la récompense apparaît immédiatement, l'ajustement de l'action est\nrelativement facile. Cependant, dans des situations plus complexes où les récompenses sont lointains,\nla situation devient plus compliquée. Comment obtenir la plus grande récompense accumulée tout au\nlong du processus? Les algorithmes d'apprentissage par renforcement "),a("span",{staticClass:"tex2jax_ignore"},[this._v("(")]),this._v("RL"),a("span",{staticClass:"tex2jax_ignore"},[this._v(")")]),this._v(" visent à résoudre ce problème d'optimisation.")])},function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("ul",[e("li",[e("em",[t._v("Environment")]),t._v(" : est l’espace, le jeu, l’environnement avec lequel la machine interagit.")]),t._v(" "),e("li",[e("em",[t._v("Agent")]),t._v(" : la machine observe l'environnement et génère une action en conséquence.")]),t._v(" "),e("li",[e("em",[t._v("Policy")]),t._v(": la règle que l'agent suit pour obtenir le but.")]),t._v(" "),e("li",[e("em",[t._v("Reward")]),t._v(": une récompense que l'agent a reçue de l'environnement pour avoir pris une action.")]),t._v(" "),e("li",[e("em",[t._v("State")]),t._v(" : l'état de l'environnement observé par l'agent.")]),t._v(" "),e("li",[e("em",[t._v("Episode")]),t._v(" : la séquence d'état et d'action jusqu'à la fin $s_1,a_1,s_2,a_2,...s_T, a_T$.")]),t._v(" "),e("li",[e("em",[t._v("Accumulative Reward")]),t._v(" : la somme de toutes les récompenses reçues d'un épisode.")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("div",{staticStyle:{width:"image width px","font-size":"80%","text-align":"center"}},[a("img",{attrs:{src:"https://i.imgur.com/nIUdsIm.jpg",align:"center"}}),this._v(" "),a("div",[this._v("Image 1: The interaction loop between agent and environment.")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("Cet exemple ci-dessous est à partir de openAI Gym, l'environnement nommé\n"),a("a",{attrs:{href:"https://github.com/openai/gym/wiki/MountainCarContinuous-v0"}},[this._v("MountaincontinuousCar-v0")]),this._v(".")])},function(){var t=this.$createElement,a=this._self._c||t;return a("div",{staticStyle:{width:"image width px","font-size":"80%","text-align":"center"}},[a("img",{staticStyle:{"padding-bottom":"0.5em"},attrs:{src:"https://i.imgur.com/yGWmDei.jpg",alt:"MountaincontinuousCar-v0"}}),this._v(" "),a("div",[this._v("Image 2: Un rendu de MountaincontinuousCar-v0.")])])},function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("ul",[e("li",[e("em",[t._v("Goal")]),t._v(" : le but de ce jeu est de trouver une policy permettant de contrôler la voiture atteignant le drapeau.")]),t._v(" "),e("li",[e("em",[t._v("Environment")]),t._v(" : des rampes et des voitures y circulent.")]),t._v(" "),e("li",[e("em",[t._v("State")]),t._v(" : l'état du véhicule a 2 dimensions, les coordonnées du véhicule sur l'axe $x$ et la vitesse du véhicule au moment de la mesure.")]),t._v(" "),e("li",[e("em",[t._v("Action")]),t._v(" : Force appliquée pour contrôler le véhicule, cependant, la force n'est pas assez forte pour pousser immédiatement la voiture au drapeau. La voiture devra faire des va-et-vient pour gagner suffisamment d’accélération et obtenir le drapeau.")]),t._v(" "),e("li",[e("em",[t._v("Reward")]),t._v(" : À chaque pas que la voiture ne peut obtenir le drapeau, l'agent reçoit une reward $r=\\frac{-a^2}{10}$,\net la reward 100 si elle atteint le cible. Alors si l'agent contrôle la voiture mais qu'il ne peut pas obtenir le drapeau, l'agent sera puni.")]),t._v(" "),e("li",[e("em",[t._v("Terminal state")]),t._v(" : si l'agent obtenir le drapeau ou si le nombre d'étapes dépasse 998 étapes.")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("div",{staticStyle:{width:"image width px","font-size":"80%","text-align":"center"}},[a("img",{attrs:{src:"https://laptrinhcuocsong.com/images/game-hung-trung.png",align:"center"}}),this._v(" "),a("div",[this._v("Image 3: le jeu Hare Egg.")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("Dans le jeu Hare Egg, supposons que nous ayons 3 actions: aller à gauche, aller à droite ou rester immobile.\nCorrespondant à l'état à ce moment $s$ "),a("span",{staticClass:"tex2jax_ignore"},[this._v(" (")]),this._v(")la position du panier, la position de l'œuf tombant contre le panier,\nla vitesse de chute des œufs..."),a("span",{staticClass:"tex2jax_ignore"},[this._v(") ")]),this._v(" nous aurons une distribution de probabilité d'action,\npar exemple $ [0.1, 0.3, 0.5] $. La somme de toutes les probabilités d'action dans l'état $s$ est $1$, nous avons: $\\sum_{a}\\pi_\\theta(a|s) = 1 $.\nSoit $p(s_{t + 1} | a_t, s_t)$ la probabilité de distribution du prochain état lorsque l'agent est à l'état $s$ et exécute une action $a$.")])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[a("strong",[this._v("L’apprentissage par renforcement vise à trouver des $\\theta$ tels que:")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("La formule montre que $\\theta^*$ est un ensemble de paramètres tels que l'attente de la reward accumulée de nombreux échantillons différents $\\tau$, que nous collectons par la policy actuelle $\\pi_\\theta$ est la plus grande. "),a("br"),this._v("\nAprès $N$ épisodes différents, l'agent collecte $N$ échantillons différents $\\tau$. La fonction objectif devient maintenant:")])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("$J(\\theta)$ est la moyenne des rewards accumulées des épisodes de $N$."),a("br"),this._v("\nNous pouvons également voir $J(\\theta)$ sous la distribution de probabilité $p_\\theta(\\tau)$ comme ci-dessous:")])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("Ce résultat est intéressant parce que la dérivée par rapport à "),a("span",{staticClass:"tex2jax_ignore"},[this._v("(")]),this._v("w.r.t"),a("span",{staticClass:"tex2jax_ignore"},[this._v(")")]),this._v(" $\\theta$ de la fonction $\\log p_\\theta(\\tau)$ ne dépend plus de la probabilité de transition de l’état $p(s_{t+1}|a_t, s_t)$, il ne dépend que de la distribution de probabilité de l’action $a_i$ exécutée par l’agent $s_i$.")])},function(){var t=this.$createElement,a=this._self._c||t;return a("ol",[a("li",[this._v("Collecte $N$ samples {$\\tau^i$} with the policy $\\pi_\\theta$")]),this._v(" "),a("li",[this._v("Calculate gradient: $\\nabla_\\theta J(\\theta) = \\frac{1}{N}\\sum_{i=1}^{N}\\bigg(\\sum_{t=1}^{t=T}\\nabla_\\theta \\log \\pi_\\theta(a_{i,t}|s_{i,t})\\bigg)\\bigg(\\sum_{t=1}^{t=T} r(a_{i,t}, s_{i,t})\\bigg) $")]),this._v(" "),a("li",[this._v("Update $\\theta \\leftarrow \\theta + \\nabla_\\theta J(\\theta)$")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("$$\n\\begin{eqnarray}\n\\nabla_\\theta J(\\theta) = \\frac{1}{N}\\sum_{i=1}^{N}\\nabla_\\theta \\log \\pi_\\theta(\\tau_i)r(\\tau_i)\n\\end{eqnarray}\n$$\nC'est exactement l'estimation du maximum de vraisemblance "),a("a",{attrs:{href:"https://fr.wikipedia.org/wiki/Maximum_de_vraisemblance"}},[this._v("EMV")]),this._v(" multipliée par la reward accumulée.\nOptimiser la fonction objectif signifie également augmenter la probabilité de suivre les trajectoires $\\tau$ qui donnent des reward cumulatives élevées.")])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("$V^\\pi(s)$: reward cumulative attendue à l'état $s$ par la policy $\\pi$."),a("br"),this._v("\n$Q^\\pi(s,a)$: récompense cumulative attendue si exécuter l'action $a$ à l'état $s$ par la policy $\\pi$."),a("br"),this._v("\nLa relation entre $V^\\pi(s)$ et $Q^\\pi(s,a)$: $V^\\pi(s) = \\sum_{a \\in A}\\pi_\\theta(s,a)Q^\\pi(s,a)$ - cela a du sens parce que $\\pi_\\theta(s,a)$ est la probabilité de faire une action $a$ à l’état $s$."),a("br"),this._v("\nNous avons également:\n$$\n\\begin{eqnarray}\nV^\\pi(s_t) &=& E_\\pi[G_t | S=s_t] \\\\\nQ^\\pi(s_t,a_t) &=& E_\\pi[G_t|S=s_t, A=a_t]\n\\end{eqnarray}\n$$\nDans lequel:"),a("br"),this._v("\nla somme de toutes les récompenses sera reçue de l'état $ s_t $ à l'avenir, avec le facteur de remise $ \\ gamma $ appelé: 0 $ <\\ gamma <1 $. Plus loin dans le futur, la récompense sera moins prise en compte, l'agent se soucie davantage des récompenses entrantes que des récompenses lointaines.")])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("Le gradient de la fonction objectif indique que l'agent fera plus d'action $a$ s'il reçoit un $Q^\\pi(s,a)$ élevé. En supposant que l'agent est à l'état $s$, le fait qu'il soit à l'état $s$ est déjà bon pour l'agent, l'exécution de toute action $a$ onnera un haut $Q^\\pi(s,a)$ il ne peut donc pas discriminer ses actions $a$ et à partir de là, il ne sait pas quelle action $a$ est optimale. Par conséquent, nous avons besoin d’une base de référence pour comparer la valeur de $Q^\\pi(s,a)$."),a("br")])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("Si $Q^\\pi(s,a)-V^\\pi(s) < 0$,2 gradients ont des signes opposés, l'optimisation de la fonction objectif réduira la probabilité d'exécution de l'action  $a$ et $s$."),a("br"),this._v("\nNous appelons $A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)$ est l'avantage de l'action $a$ à l'état $s$.")])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("Stochastic Actor signifie que la policy $\\pi_\\theta(a|s)$ est une distribution de probabilité d'actions à $s$. Nous appelons Stochastic Actor pour le distinguer de Deterministic Actor "),a("span",{staticClass:"tex2jax_ignore"},[this._v("(")]),this._v("ou Deterministic Policy"),a("span",{staticClass:"tex2jax_ignore"},[this._v(")")]),this._v(" ce qui signifie que la politique n'est pas une distribution de probabilité d'actions à $s$, mais sous $s$ nous n'exécutons qu'une action déterministe. En d'autres termes, la probabilité d'exécution d'une action choisie $a=\\mu_\\theta(s)$ sur $s$ vaut 1 et toutes les autres actions valent 0.")])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("La fonction objectif dépend de 2 choses: la fonction policy et value $ V ^ \\ pi $. En supposant que nous ayons une fonction d’approximation pour $ V ^ \\ pi (s) $ est $ V_ \\ phi (s) $ en fonction des paramètres $ \\ phi $. "),a("br"),this._v("\nNous appelons la fonction d'approbation de la politique $ \\ pi_ \\ theta $ Actor et la fonction d'appromixation de $ V_ \\ phi $ est Critique.")])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("La fonction objectif dépend de 2 choses: policy $\\pi_\\theta$ et la function value $V^\\pi$. En supposant que nous ayons une fonction d’approximation pour $V^\\pi(s)$ is $V_\\phi(s)$ en fonction des paramètres $\\phi$."),a("br"),this._v("\nNous appelons la fonction d'd’approximation de la policy $\\pi_\\theta$ est Actor  et la fonction d'appromixation de $V_\\phi$ est Critic.")])},function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("p",[t._v("Batch Actor-Critic:"),e("br"),t._v("\n1. Collecte une trajectoire $\\tau$ à l'état terminal par la policy $\\pi_\\theta$"),e("br"),t._v("\n2. Fit $V_\\phi$ avec $y = \\sum_{i}^{T} r_i$"),e("br"),t._v("\n3. Calcul $A(s_t,a_t) = r(s_t, a_t) + \\gamma V_\\phi(s_{t+1}) - V_\\phi(s_{t})$"),e("br"),t._v("\n4. Calcul $\\nabla_\\theta J(\\theta) = \\sum_i \\nabla \\log \\pi_\\theta (a_i|s_i) A^\\pi (s_i, a_i)$"),e("br"),t._v("\n5. Update $\\theta \\leftarrow \\theta  + \\alpha \\nabla_\\theta J(\\theta)$"),e("br"),t._v(" "),e("br"),t._v(" "),e("br"),t._v("\nCi-dessus, nous pouvons représenter $V_\\phi(s) = r + V_\\phi(s')$ selon l'équation de Bellman, Nous pouvons donc mettre à jour le modèle en ne sachant qu'une seule étape."),e("br"),t._v("\nOnline Actor-Critic:"),e("br"),t._v("\n1. Avec policy $\\pi_\\theta$, exécutez 1 action $a \\sim \\pi_\\theta(a|s)$ pour avoir $(s,a,s',r)$"),e("br"),t._v("\n2. Fit $V_\\phi (s)$ avec $r + V_\\phi(s')$"),e("br"),t._v("\n3. Trouvez $A(s_t,a_t) = r(s_t, a_t) + \\gamma V_\\phi(s_{t+1}) - V_\\phi(s_{t})$"),e("br"),t._v("\n4. Trouvez $\\nabla_\\theta J(\\theta) = \\sum_i \\nabla \\log \\pi_\\theta (a_i|s_i) A (s_i, a_i)$"),e("br"),t._v("\n5. Mettre à jour $\\theta \\leftarrow \\theta  + \\alpha \\nabla_\\theta J(\\theta)$"),e("br"),t._v(" "),e("br"),t._v(" "),e("br"),t._v("\nAlors, nous mettons à jour de manière itérative les deux fonctions d’approximation $V_\\phi$ et $\\pi_\\theta$.")])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("Examiner une policy comme suit:\n$$\n\\begin{eqnarray}\n\\pi'(a_t|s_t) = 1 \\ \\text{if}\\  a_t = \\arg \\max_{a_t} A^\\pi(s_t, a_t)\n\\end{eqnarray}\n$$\nPolicy $\\pi'$ est une Deterministic Policy: étant donné une policy $\\pi$ et en supposant que nous connaissons l'avantage des actions à l'état $s_t$ sous la policy $\\pi$, nous choisissons toujours l'action avec le plus grand avantage à l'état $s$, la probabilité de cette action est 1, toutes les autres actions à $s_t$ sont 0.\nLa policy $\\pi'$ sera toujours meilleure ou au moins égale à la policy $\\pi$. Une policy est évaluée égale ou meilleure que les autres si:\n$V^\\pi(s) \\leq V^{\\pi'} (s) \\forall s \\in S$ : avec tout l'état $s$ dans l'espace d'état $S$, la valeur de retour $V^\\pi(s)$ toujours inférieur ou égal à la valeur de retour $V^{\\pi'} (s)$."),a("br"),this._v("\nPar exemple, nous avons: à l'état $s$, nous avons 4 façons de passer à l'état $s'$ correspondant à 4 actions et avantages $A^\\pi_1$, $A^\\pi_2$, $A^\\pi_3$, $A^\\pi_4$.De l'état $s'$, nous continuons à suivre la policy $\\pi$. De $s$ à $s'$, si nous choisissons de suivre la stochastic policy $\\pi$,  l’avantage expecté est de $\\sum_{a \\in A} p(a)A^\\pi_a$, cette quantité doit être inférieure à ou égal à $\\max_a A^\\pi_a$.")])},function(){var t=this.$createElement,a=this._self._c||t;return a("div",{staticStyle:{width:"image width px","font-size":"80%","text-align":"center"}},[a("img",{attrs:{src:"https://i.imgur.com/yMtTahR.jpg",align:"center"}}),this._v(" "),a("div",[this._v("Image 4: Passage de l'état $s$ à $s'$.")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("But evaluating $A^\\pi(s,a)$ is also equivalent to evaluate $Q^\\pi(s,a)$ because $A^\\pi(s,a) =  Q^\\pi(s,a) - V^\\pi(s) = r(s,a)  + \\gamma V^\\pi(s') - V^\\pi(s)$, and the quantity $V^\\pi(s)$ is the same for different actions $a$ at state $s$."),a("br"),this._v("\nThe algorithm now becomes:")])},function(){var t=this.$createElement,a=this._self._c||t;return a("ol",[a("li",[this._v("Evaluate $Q^\\pi(s,a) \\leftarrow r(s,a)  + \\gamma V^\\pi(s') $ với các action $a$ khác nhau")]),this._v(" "),a("li",[this._v("Optimize $\\pi \\leftarrow \\pi'$ : choose the action with highest $A$, it is also highest $Q$")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("ol",[a("li",[this._v("Evaluate $Q^\\pi(s,a) \\leftarrow r(s,a)  + \\gamma V^\\pi(s') $ for different actions $a$")]),this._v(" "),a("li",[this._v("$V^\\pi(s) \\leftarrow \\max_a Q^\\pi(s,a)$")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("ol",[a("li",[this._v("Evaluate $V^\\pi(s) \\leftarrow \\max_a \\big(r(s,a)  + \\gamma V^\\pi(s')\\big)$")]),this._v(" "),a("li",[this._v("$\\phi \\leftarrow \\arg min_\\phi \\big(V^\\pi(s) - V_\\phi(s)\\big)^2$")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("ol",[a("li",[this._v("Evaluate $y_i \\leftarrow r(s,a_i)  + \\gamma \\max_{a'} Q_\\phi(s', a') $")]),this._v(" "),a("li",[this._v("$\\phi \\leftarrow \\arg min_\\phi \\big(Q_\\phi(s, a_i) - y_i\\big)^2$ $(*)$")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("This is the Q-Learning algorithm. Notice that, reward $r$ above is not dependent on the state transition and on the policy $\\pi$ used to generate the sample, therefore we only need the sample $(s, a, r, s')$ to improve the policy without knowing that sample is generated from which policy. Because of this reason, we call it off-policy. Later, we will have on-policy algorithms and they need the new samples generated by the current policy to improve itself "),a("span",{staticClass:"tex2jax_ignore"},[this._v("(")]),this._v("cannot use experience from history"),a("span",{staticClass:"tex2jax_ignore"},[this._v(")")]),this._v(".\nWe have the Online Q-Learning as follow:")])},function(){var t=this.$createElement,a=this._self._c||t;return a("ol",[a("li",[this._v("Evaluate action $a$ to have $(s, a, s', r)$")]),this._v(" "),a("li",[this._v("Evaluate $y_i \\leftarrow r(s,a_i)  + \\gamma max_{a'} Q_\\phi(s', a') $")]),this._v(" "),a("li",[this._v("$\\phi \\leftarrow \\phi - \\alpha \\frac{dQ_\\phi}{d\\phi}(s,a) \\big(Q_\\phi(s, a_i) - y_i\\big)$")])])},function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("p",[t._v("Notice in the step 3, is it the gradient descent as same as where I marked $(*)$ above? The answer is no, in fact, we ignore the derivative of $y_i$ by $\\phi$ "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("$y_i$ also depends on $\\phi$"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(". As a consequence, everytime we update $\\phi$ with this algorithm, the value of the target $y_i$ is also changed! The target changes when we try to get closer, this makes the algorithm becomes unstable."),e("br"),t._v("\nTo solve this, we need another approximation function called target network, different from the train network we use to run. Target network will be update slowly and is used to evaluate $y$."),e("br"),t._v("\nAnother problem is that samples are generated continuously so they are correlated. The algorithm above is similar as Supervised Learning - we map a Q-value with a target, we want the samples are independent and identically distributed "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("i.i.d"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(". To break the correlation between samples, we could use an experience buffer: a list containing many samples from different episodes and we choose randomly a batch from the buffer and train our agent on that batch.")])},function(){var t=this.$createElement,a=this._self._c||t;return a("ul",[a("li",[this._v("A separated target network called $Q_{\\phi'}$.")]),this._v(" "),a("li",[this._v("Experience buffer.")])])},function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("ol",[e("li",[t._v("Execute action $a_i$ to have $(s_i, a_i, s_i', r_i)$ and put it into the buffer.")]),t._v(" "),e("li",[t._v("Sample randomly a batch $N$ samples from the buffer $(s_i, a_i, s_i', r_i)$.")]),t._v(" "),e("li",[t._v("Evaluate $y_i \\leftarrow r(s,a_i)  + \\gamma max_{a'} Q_{\\phi'}(s', a')$ "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("using target network here"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")])]),t._v(" "),e("li",[t._v("$\\phi \\leftarrow \\phi - \\alpha \\frac{1}{N}\\sum_i^N \\frac{dQ_\\phi}{d\\phi}(s,a_i) \\big(Q_\\phi(s, a_i) - y_i\\big)$")]),t._v(" "),e("li",[t._v("Update target network $\\phi' \\leftarrow (1-\\tau)\\phi' + \\tau \\phi$ "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("using $\\tau %$ of new train network to update target network"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(" "),e("br"),t._v(" "),e("br"),t._v("\nThis algorithm is Deep Q-Network "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("DQN"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(".")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("Deep Deterministic Policy Gradient "),a("span",{staticClass:"tex2jax_ignore"},[this._v("(")]),this._v("DDPG"),a("span",{staticClass:"tex2jax_ignore"},[this._v(")")]),this._v(" has a nice approach, notice that:\n$$\n\\begin{eqnarray}\nmax_{a} Q_{\\phi}(s, a) = Q_\\phi\\big(s, \\arg max_a Q_\\phi(s,a)\\big)\n\\end{eqnarray}\n$$")])},function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("ol",[e("li",[t._v("Execute action $a_i$ to have $(s_i, a_i, s_i', r_i)$ and put it into the buffer.")]),t._v(" "),e("li",[t._v("Sample a batch of $N$ samples from the buffer $(s_i, a_i, s_i', r_i)$.")]),t._v(" "),e("li",[t._v("Evaluate $y_i \\leftarrow r(s,a_i)  + \\gamma Q_{\\phi'}\\big(s', \\mu_{\\theta'}(s')\\big)$ "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("use both policy và Q target network here"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")])]),t._v(" "),e("li",[t._v("$\\phi \\leftarrow \\phi - \\alpha \\frac{1}{N}\\sum_i^N \\frac{dQ_\\phi}{d\\phi}(s,a_i) \\big(Q_\\phi(s, a_i) - y_i\\big)$")]),t._v(" "),e("li",[t._v("$\\theta \\leftarrow \\theta - \\beta \\frac{1}{N}\\sum_i^N \\frac{d\\mu_\\theta}{d\\theta}(s) \\frac{dQ_\\phi}{da}(s, a_i)$")]),t._v(" "),e("li",[t._v("Update target network $\\phi' \\leftarrow (1-\\tau)\\phi' + \\tau \\phi$ và $\\theta' \\leftarrow (1-\\tau)\\theta' + \\tau \\theta$\n"),e("br"),t._v(" "),e("br"),t._v("\nNotice in the  DDPG implementation:")])])},function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("ul",[e("li",[t._v("Because actions in DDPG are always deterministic, thus to explore the environment "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("we do not want the agent always exploit the best trajectory in its knowledge, there may be a better path out there"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(", a small action noise will be added into the action from the agent.\nThe noise in the "),e("a",{attrs:{href:"https://arxiv.org/abs/1509.02971"}},[t._v("original paper")]),t._v(" is a stochastic process named Ornstein–Uhlenbeck process "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("OU process"),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v(". The authors choosed this process because the experiments gave good result, however other experiments conducted by different groups showing that other noise such as Gaussian Noise give the same performance.")]),t._v(" "),e("li",[t._v("Implementation of action noise in the library "),e("a",{attrs:{href:"https://github.com/keras-rl/keras-rl"}},[t._v("keras-rl")]),t._v(" is OU process, however when I run, this noise is not decayed w.r.t time. Hoever, we need a big noise at the beginning "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("to explore the environment"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(" and decrease after many episodes. This can be done if before we add the noise to the action, we multiply it with a quantity epsilon and the epsilon decays to 0 w.r.t time.")]),t._v(" "),e("li",[t._v("Besides adding noise into the action before executing it on the environment, they also add Gaussian Noise on the node of Neural Network. Reference paper "),e("a",{attrs:{href:"https://openai.com/blog/better-exploration-with-parameter-noise/"}},[t._v("here")]),t._v(". You could find the implementation o actor network noise in this library "),e("a",{attrs:{href:"https://stable-baselines.readthedocs.io/en/master/modules/ddpg.html#action-and-parameters-noise"}},[t._v("stable baselines")]),t._v(".")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("ul",[a("li",[this._v("Policy Gradient is an on-policy and a stochastic policy.")]),this._v(" "),a("li",[this._v("Q-learning, DQN, DDPG are off-policy and deterministic policies.")]),this._v(" "),a("li",[this._v("Always have a deterministic policy better than a stochastic policy.")])])}]},n=e("VU/8")(null,i,!1,null,null,null);a.default=n.exports},Kwpa:function(t,a,e){"use strict";Object.defineProperty(a,"__esModule",{value:!0});var i={components:{SideMenu:e("pukS").a}},n={render:function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("div",[e("nav",{staticClass:"navbar is-fixed-top is-light is-bold"},[t._m(0),t._v(" "),e("div",{staticClass:"navbar-menu",attrs:{id:"navbarLanguage"}},[e("div",{staticClass:"navbar-start"},[e("div",{staticClass:"navbar-item has-dropdown is-hoverable"},[e("a",{staticClass:"navbar-link"},[t._v("\n                      Langue\n                  ")]),t._v(" "),e("div",{staticClass:"navbar-dropdown is-boxed"},[e("a",{staticClass:"navbar-item"},[e("router-link",{attrs:{to:"/"}},[e("span",[t._v("Anglais")])])],1),t._v(" "),e("a",{staticClass:"navbar-item"},[e("router-link",{attrs:{to:"/vi"}},[e("span",[t._v("Vietnamien")])])],1),t._v(" "),e("a",{staticClass:"navbar-item"},[e("router-link",{attrs:{to:"/fr"}},[e("span",[t._v("Français")])])],1)])])]),t._v(" "),t._m(1)])]),t._v(" "),e("section",{staticClass:"section"},[e("div",{staticClass:"container"},[e("div",{staticClass:"columns"},[e("div",{staticClass:"column is-2 side-nav"},[e("side-menu")],1),t._v(" "),e("div",{staticClass:"column is-10 markdown"},[e("router-view")],1)])])]),t._v(" "),t._m(2)])},staticRenderFns:[function(){var t=this.$createElement,a=this._self._c||t;return a("div",{staticClass:"navbar-brand"},[a("a",{staticClass:"navbar-item",attrs:{href:""}},[a("span",{staticClass:"logo"},[this._v("\n                  Frew"),a("i",{staticClass:"fas fa-angle-double-up"}),this._v("rd\n              ")])]),this._v(" "),a("div",{staticClass:"navbar-burger burger",attrs:{"data-target":"navbarLanguage"}},[a("span"),this._v(" "),a("span"),this._v(" "),a("span")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("div",{staticClass:"navbar-end"},[a("div",{staticClass:"navbar-item"},[a("div",{staticClass:"field is-grouped"},[a("p",{staticClass:"control"},[a("a",{staticClass:"bd-tw-button button",attrs:{href:"https://github.com/toanngosy/"}},[a("span",{staticClass:"icon"},[a("i",{staticClass:"fab fa-github"})])])]),this._v(" "),a("p",{staticClass:"control"},[a("a",{staticClass:"bd-tw-button button",attrs:{href:"https://www.linkedin.com/in/sy-toan-ngo-491109bb/"}},[a("span",{staticClass:"icon"},[a("i",{staticClass:"fab fa-linkedin"})])])]),this._v(" "),a("p",{staticClass:"control"},[a("a",{staticClass:"bd-tw-button button",attrs:{href:"https://www.facebook.com/toanngosy/"}},[a("span",{staticClass:"icon"},[a("i",{staticClass:"fab fa-facebook"})])])])])])])},function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("footer",{staticClass:"footer"},[e("div",{staticClass:"container"},[e("div",{staticClass:"content has-text-centered"},[e("p",[e("strong",[t._v("Frew"),e("i",{staticClass:"fas fa-angle-double-up"}),t._v("rd")]),t._v(" par "),e("a",{attrs:{href:"https://www.facebook.com/toanngosy/",target:"_blank"}},[t._v("Gonaton")]),t._v(". Le code source est sous licence\n          "),e("a",{attrs:{href:"http://opensource.org/licenses/mit-license.php"}},[t._v("MIT")]),t._v(".\n        ")]),t._v(" "),e("p",[e("a",{staticClass:"icon",attrs:{href:"https://github.com/toanngosy/"}},[e("i",{staticClass:"fab fa-github"})])])])])])}]};var s=e("VU/8")(i,n,!1,function(t){e("PGeF")},"data-v-3d0ea01e",null);a.default=s.exports},NHnr:function(t,a,e){"use strict";Object.defineProperty(a,"__esModule",{value:!0});var i=e("7+uW"),n={render:function(){var t=this.$createElement,a=this._self._c||t;return a("div",{attrs:{id:"app"}},[a("router-view")],1)},staticRenderFns:[]};var s=e("VU/8")({name:"App"},n,!1,function(t){e("YDcN")},null,null).exports,r=e("/ocq"),o=[{path:"/",component:e("v53a").default,children:[{path:"",component:e("E8C0").default,children:[{path:"",component:e("7ZQQ").default}]},{path:"theorical",component:e("Phj7").default,children:[{path:"1",component:e("PPBh").default}]}]}],_=[{path:"/vi",component:e("0U7u").default,children:[{path:"",component:e("eDZt").default,children:[{path:"",component:e("BVxU").default}]},{path:"theorical",component:e("4jvi").default,children:[{path:"1",component:e("SZf6").default}]}]}],c=[{path:"/fr",component:e("xD4a").default,children:[{path:"",component:e("DC16").default,children:[{path:"",component:e("DPxC").default}]},{path:"theorical",component:e("Kwpa").default,children:[{path:"1",component:e("WXZB").default}]}]}];i.a.use(r.a);var l=[];l=(l=(l=l.concat(o)).concat(_)).concat(c);var h=new r.a({routes:l}),v=e("osNU");i.a.use(v.default),i.a.config.productionTip=!1,new i.a({mode:"history",el:"#app",router:h,components:{App:s},template:"<App/>"})},PGeF:function(t,a){},PPBh:function(t,a,e){t.exports=e("2wRn")},Phj7:function(t,a,e){"use strict";Object.defineProperty(a,"__esModule",{value:!0});var i={components:{SideMenu:e("C05T").a}},n={render:function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("div",[e("nav",{staticClass:"navbar is-fixed-top is-light is-bold"},[t._m(0),t._v(" "),e("div",{staticClass:"navbar-menu",attrs:{id:"navbarLanguage"}},[e("div",{staticClass:"navbar-start"},[e("div",{staticClass:"navbar-item has-dropdown is-hoverable"},[e("a",{staticClass:"navbar-link"},[t._v("\n                      Language\n                  ")]),t._v(" "),e("div",{staticClass:"navbar-dropdown is-boxed"},[e("a",{staticClass:"navbar-item"},[e("router-link",{attrs:{to:"/"}},[e("span",[t._v("English")])])],1),t._v(" "),e("a",{staticClass:"navbar-item"},[e("router-link",{attrs:{to:"/vi"}},[e("span",[t._v("Vietnamese")])])],1),t._v(" "),e("a",{staticClass:"navbar-item"},[e("router-link",{attrs:{to:"/fr"}},[e("span",[t._v("French")])])],1)])])]),t._v(" "),t._m(1)])]),t._v(" "),e("section",{staticClass:"section"},[e("div",{staticClass:"container"},[e("div",{staticClass:"columns"},[e("div",{staticClass:"column is-2 side-nav"},[e("side-menu")],1),t._v(" "),e("div",{staticClass:"column is-10 markdown"},[e("router-view")],1)])])]),t._v(" "),t._m(2)])},staticRenderFns:[function(){var t=this.$createElement,a=this._self._c||t;return a("div",{staticClass:"navbar-brand"},[a("a",{staticClass:"navbar-item",attrs:{href:""}},[a("span",{staticClass:"logo"},[this._v("\n                  Frew"),a("i",{staticClass:"fas fa-angle-double-up"}),this._v("rd\n              ")])]),this._v(" "),a("div",{staticClass:"navbar-burger burger",attrs:{"data-target":"navbarLanguage"}},[a("span"),this._v(" "),a("span"),this._v(" "),a("span")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("div",{staticClass:"navbar-end"},[a("div",{staticClass:"navbar-item"},[a("div",{staticClass:"field is-grouped"},[a("p",{staticClass:"control"},[a("a",{staticClass:"bd-tw-button button",attrs:{href:"https://github.com/toanngosy/"}},[a("span",{staticClass:"icon"},[a("i",{staticClass:"fab fa-github"})])])]),this._v(" "),a("p",{staticClass:"control"},[a("a",{staticClass:"bd-tw-button button",attrs:{href:"https://www.linkedin.com/in/sy-toan-ngo-491109bb/"}},[a("span",{staticClass:"icon"},[a("i",{staticClass:"fab fa-linkedin"})])])]),this._v(" "),a("p",{staticClass:"control"},[a("a",{staticClass:"bd-tw-button button",attrs:{href:"https://www.facebook.com/toanngosy/"}},[a("span",{staticClass:"icon"},[a("i",{staticClass:"fab fa-facebook"})])])])])])])},function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("footer",{staticClass:"footer"},[e("div",{staticClass:"container"},[e("div",{staticClass:"content has-text-centered"},[e("p",[e("strong",[t._v("Frew"),e("i",{staticClass:"fas fa-angle-double-up"}),t._v("rd")]),t._v(" by "),e("a",{attrs:{href:"https://www.facebook.com/toanngosy/",target:"_blank"}},[t._v("Gonaton")]),t._v(". The source code is licensed\n          "),e("a",{attrs:{href:"http://opensource.org/licenses/mit-license.php"}},[t._v("MIT")]),t._v(".\n        ")]),t._v(" "),e("p",[e("a",{staticClass:"icon",attrs:{href:"https://github.com/toanngosy/"}},[e("i",{staticClass:"fab fa-github"})])])])])])}]};var s=e("VU/8")(i,n,!1,function(t){e("fTrl")},"data-v-f1e1833e",null);a.default=s.exports},SZf6:function(t,a,e){t.exports=e("zNHS")},WXZB:function(t,a,e){t.exports=e("Kjo4")},YDcN:function(t,a){},b5jy:function(t,a){},eDZt:function(t,a,e){"use strict";Object.defineProperty(a,"__esModule",{value:!0});var i={components:{SideMenu:e("9fpD").a}},n={render:function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("div",[e("nav",{staticClass:"navbar is-fixed-top is-light is-bold"},[t._m(0),t._v(" "),e("div",{staticClass:"navbar-menu",attrs:{id:"navbarLanguage"}},[e("div",{staticClass:"navbar-start"},[e("div",{staticClass:"navbar-item has-dropdown is-hoverable"},[e("a",{staticClass:"navbar-link"},[t._v("\n                      Ngôn Ngữ\n                  ")]),t._v(" "),e("div",{staticClass:"navbar-dropdown is-boxed"},[e("a",{staticClass:"navbar-item"},[e("router-link",{attrs:{to:"/"}},[e("span",[t._v("Tiếng Anh")])])],1),t._v(" "),e("a",{staticClass:"navbar-item"},[e("router-link",{attrs:{to:"/vi"}},[e("span",[t._v("Tiếng Việt")])])],1),t._v(" "),e("a",{staticClass:"navbar-item"},[e("router-link",{attrs:{to:"/fr"}},[e("span",[t._v("Tiếng Pháp")])])],1)])])]),t._v(" "),t._m(1)])]),t._v(" "),t._m(2),t._v(" "),e("section",{staticClass:"section"},[e("div",{staticClass:"container"},[e("div",{staticClass:"columns"},[e("div",{staticClass:"column is-2 side-nav"},[e("side-menu")],1),t._v(" "),e("div",{staticClass:"column is-10 markdown"},[e("router-view")],1)])])]),t._v(" "),t._m(3)])},staticRenderFns:[function(){var t=this.$createElement,a=this._self._c||t;return a("div",{staticClass:"navbar-brand"},[a("a",{staticClass:"navbar-item",attrs:{href:""}},[a("span",{staticClass:"logo"},[this._v("\n                  Frew"),a("i",{staticClass:"fas fa-angle-double-up"}),this._v("rd\n              ")])]),this._v(" "),a("div",{staticClass:"navbar-burger burger",attrs:{"data-target":"navbarLanguage"}},[a("span"),this._v(" "),a("span"),this._v(" "),a("span")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("div",{staticClass:"navbar-end"},[a("div",{staticClass:"navbar-item"},[a("div",{staticClass:"field is-grouped"},[a("p",{staticClass:"control"},[a("a",{staticClass:"bd-tw-button button",attrs:{href:"https://github.com/toanngosy/"}},[a("span",{staticClass:"icon"},[a("i",{staticClass:"fab fa-github"})])])]),this._v(" "),a("p",{staticClass:"control"},[a("a",{staticClass:"bd-tw-button button",attrs:{href:"https://www.linkedin.com/in/sy-toan-ngo-491109bb/"}},[a("span",{staticClass:"icon"},[a("i",{staticClass:"fab fa-linkedin"})])])]),this._v(" "),a("p",{staticClass:"control"},[a("a",{staticClass:"bd-tw-button button",attrs:{href:"https://www.facebook.com/toanngosy/"}},[a("span",{staticClass:"icon"},[a("i",{staticClass:"fab fa-facebook"})])])])])])])},function(){var t=this.$createElement,a=this._self._c||t;return a("section",{staticClass:"hero is-dark is-medium"},[a("div",{staticClass:"hero-body"},[a("div",{staticClass:"container"},[a("span",{staticClass:"randomimage"},[a("i",{staticClass:"fas fa-brain"})]),this._v(" "),a("span",{staticClass:"welcome"},[this._v("Frew"),a("i",{staticClass:"fas fa-angle-double-up"}),this._v("rd chào bạn")])])])])},function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("footer",{staticClass:"footer"},[e("div",{staticClass:"container"},[e("div",{staticClass:"content has-text-centered"},[e("p",[e("strong",[t._v("Frew"),e("i",{staticClass:"fas fa-angle-double-up"}),t._v("rd")]),t._v(" by "),e("a",{attrs:{href:"https://www.facebook.com/toanngosy/",target:"_blank"}},[t._v("Gonaton")]),t._v(". Mã nguồn bản quyền\n          "),e("a",{attrs:{href:"http://opensource.org/licenses/mit-license.php"}},[t._v("MIT")]),t._v(".\n        ")]),t._v(" "),e("p",[e("a",{staticClass:"icon",attrs:{href:"https://github.com/toanngosy/"}},[e("i",{staticClass:"fab fa-github"})])])])])])}]};var s=e("VU/8")(i,n,!1,function(t){e("jift")},"data-v-b20430e4",null);a.default=s.exports},fTrl:function(t,a){},jift:function(t,a){},olKP:function(t,a){},pukS:function(t,a,e){"use strict";var i={render:function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("aside",{staticClass:"list-group"},[e("ul",[e("li",[e("router-link",{staticClass:"list-group-item",attrs:{to:"/",exact:""}},[t._v("Introduction")])],1)]),t._v(" "),e("div",{staticClass:"menu"},[e("p",{staticClass:"menu-label"},[t._v("RL Théorique")]),t._v(" "),e("ul",{staticClass:"menu-list"},[e("li",[e("router-link",{attrs:{to:"/fr/theorical/1",exact:""}},[t._v("DDPG")])],1),t._v(" "),e("li",[e("router-link",{attrs:{to:"/en/components/layout",exact:""}},[t._v("TRPO/PPO")])],1),t._v(" "),e("li",[e("router-link",{attrs:{to:"/en/components/button",exact:""}},[t._v("SAC")])],1),t._v(" "),e("li",[e("router-link",{attrs:{to:"/en/components/icon",exact:""}},[t._v("Inference")])],1)]),t._v(" "),e("p",{staticClass:"menu-label"},[t._v("RL Practique")]),t._v(" "),e("ul",{staticClass:"menu-list"},[e("li",[e("router-link",{attrs:{to:"/en/components/affix",exact:""}},[t._v("OpenAI Gym")])],1),t._v(" "),e("li",[e("router-link",{attrs:{to:"/en/components/scrollto",exact:""}},[t._v("Q-learning")])],1),t._v(" "),e("li",[e("router-link",{attrs:{to:"/en/components/breadcrumb",exact:""}},[t._v("DDPG")])],1),t._v(" "),e("li",[e("router-link",{attrs:{to:"/en/components/breadcrumb",exact:""}},[t._v("TRPO/PPO")])],1),t._v(" "),e("li",[e("router-link",{attrs:{to:"/en/components/breadcrumb",exact:""}},[t._v("SAC")])],1)]),t._v(" "),e("p",{staticClass:"menu-label"},[t._v("Révision bibliothèque ")]),t._v(" "),e("ul",{staticClass:"menu-list"},[e("li",[e("router-link",{attrs:{to:"/en/components/alert",exact:""}},[t._v("OpenAI baselines")])],1),t._v(" "),e("li",[e("router-link",{attrs:{to:"/en/components/aside",exact:""}},[t._v("Stable baselines")])],1),t._v(" "),e("li",[e("router-link",{attrs:{to:"/en/components/collapse",exact:""}},[t._v("Ray/RLLib")])],1),t._v(" "),e("li",[e("router-link",{attrs:{to:"/en/components/modal",exact:""}},[t._v("Unity ML-Agents")])],1)]),t._v(" "),e("p",{staticClass:"menu-label"},[t._v("Application")]),t._v(" "),e("ul",{staticClass:"menu-list"},[e("li",[e("router-link",{attrs:{to:"/en/components/form",exact:""}},[t._v("Frew"),e("i",{staticClass:"fas fa-angle-double-up"}),t._v("rd Env")])],1),t._v(" "),e("li",[e("router-link",{attrs:{to:"/en/components/form2",exact:""}},[t._v("Frew"),e("i",{staticClass:"fas fa-angle-double-up"}),t._v("rd Exp")])],1)])])])},staticRenderFns:[]};var n=e("VU/8")(null,i,!1,function(t){e("E6CH")},"data-v-7d7cdfd8",null);a.a=n.exports},tz1K:function(t,a,e){"use strict";Object.defineProperty(a,"__esModule",{value:!0});var i={render:function(){this.$createElement;this._self._c;return this._m(0)},staticRenderFns:[function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("section",[e("h1",[t._v("Lời mở đầu")]),t._v(" "),e("p",[t._v("Bên cạnh sự phát triển của Computer Vision, Natural Language Processing,\ncác thuật toán Reinforcement Learning cũng đạt được những thành tựu đáng\nngạc nhiên trong những năm gần đây và nhận được nhiều sự chú ý.\nTuy nhiên, các bài hướng dẫn được tìm thấy trên mạng thường đẩy nhanh đến\nphần code mà không đi sâu vào toán.\nBlog này dựa trên bài giảng của giáo sư Sergey Levine thuộc đại học UC Berkeley\nvà sách Introduction to Reinforcement Learning của giáo sư Sutton và Barto,\ncác bạn có thể tìm thấy link cho bài giảng\n"),e("a",{attrs:{href:"http://rail.eecs.berkeley.edu/deeprlcourse/"}},[t._v("tại đây")]),t._v("\nvà sách "),e("a",{attrs:{href:"http://incompleteideas.net/book/bookdraft2017nov5.pdf"}},[t._v("tại đây")]),t._v(".")]),t._v(" "),e("p",[t._v("Blog cũng sẽ giới thiệu các thư viện RL và cách dùng chúng trong\nthực tế.")]),t._v(" "),e("h1",[t._v("Về Frew"),e("i",{staticClass:"fas fa-angle-double-up"}),t._v("rd")]),t._v(" "),e("p",[t._v("Trong RL, reward rất quan trọng, các hành động mà máy học được là để\ntối ưu tổng lượng reward về lâu dài và xây dựng hàm reward sao cho hợp lý\nlà một trong các điều kiện cần để thuật toán RL có thể chạy được. Trong thực tế,\nreward cũng rất quan trọng, đó chính là mục tiêu mà chúng ta hướng đến\ntrong cuộc sống.\nHàm reward - \\(f(reward)\\) - Frew"),e("i",{staticClass:"fas fa-angle-double-up"}),t._v("rd\nxuất phát từ đó.\n"),e("br"),t._v(" "),e("br"),t._v(" "),e("br"),t._v(" "),e("br")]),t._v(" "),e("p",{staticStyle:{"font-size":"200%"},attrs:{align:"center"}},[t._v("In this Freeworld, what is your Frew"),e("i",{staticClass:"fas fa-angle-double-up"}),t._v("rd?")])])}]},n=e("VU/8")(null,i,!1,null,null,null);a.default=n.exports},v53a:function(t,a,e){"use strict";Object.defineProperty(a,"__esModule",{value:!0});var i={render:function(){var t=this.$createElement;return(this._self._c||t)("router-view")},staticRenderFns:[]},n=e("VU/8")(null,i,!1,null,null,null);a.default=n.exports},xD4a:function(t,a,e){"use strict";Object.defineProperty(a,"__esModule",{value:!0});var i={render:function(){var t=this.$createElement;return(this._self._c||t)("router-view")},staticRenderFns:[]},n=e("VU/8")(null,i,!1,null,null,null);a.default=n.exports},zNHS:function(t,a,e){"use strict";Object.defineProperty(a,"__esModule",{value:!0});var i={render:function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("section",[e("p",[e("vue-mathjax")],1),t._v(" "),e("h1",[t._v("I - Reinforcement Learning - từ Policy Gradient đến Deep Deterministic Policy Gradient")]),t._v(" "),t._m(0),t._v(" "),t._m(1),t._v(" "),e("p",[t._v("Dưới đây là định nghĩa của các thuật ngữ hay xuất hiện trong RL:")]),t._v(" "),t._m(2),t._v(" "),t._m(3),t._v(" "),e("br"),t._v(" "),e("p",[t._v("Như vậy, tại state $s$, agent tương tác với environment với hành động $a$,\ndẫn đến state mới $s_{t+1}$ và nhận được reward tương ứng $r_{t+1}$.\nVòng lặp như thế cho đến trạng thái cuối cùng $s_T$.")]),t._v(" "),e("p",[t._v("Trong phần dưới đây, mình sẽ dùng các thuật ngữ tiếng Anh để các bạn tiện\ntheo dõi thay vì dịch sang tiếng Việt.")]),t._v(" "),e("h1",[t._v("1 - Ví dụ")]),t._v(" "),t._m(4),t._v(" "),t._m(5),t._v(" "),e("br"),t._v("\n* _Goal_: mục đích của bài toán là xây dựng policy để điều khiển xe lên đến được chỗ cắm cờ.\n* _Environment_: dốc và xe chạy trong đó\n* _State_: trạng thái của xe có 2 dimension, tọa độ của xe theo trục $x$ và vận tốc của xe tại thời điểm đo.\n* _Action_: Lực được truyền cho xe để điều khiển, lực này không đủ mạnh\nđể đẩy xe 1 lúc lên đến cờ, xe sẽ cần đi qua đi lại 2 bên mặt nghiên để\ntăng tốc đến chỗ cắm cờ.\n* _Reward_: với mỗi step mà xe không đến được cờ, agent nhận reward $r=\\frac{-a^2}{10}$,\nxe đến được cờ thì nhận reward là 100. Như thế, nếu agent điều khiển xe mà xe không lên được thì sẽ bị phạt.\n* _Terminal state_: nếu agent lên đến được cờ hoặc là số step vượt quá 998 steps.\n"),e("h1",[t._v("2 - Policy Gradient")]),t._v(" "),e("p",[t._v("Trong phần dưới đây để ví dụ sinh động, mình giải quyết 1 bài toán game đơn giản, game Hứng Trứng.")]),t._v(" "),t._m(6),t._v(" "),e("br"),t._v(" "),e("p",[t._v("Gọi $\\pi_\\theta(a|s) = f(s, \\theta)$ là policy của agent, đó hàm phân bố xác suất của action $a$ tại state $s$.")]),t._v(" "),t._m(7),t._v(" "),e("p",[t._v("Gọi $\\tau = s_1, a_1, s_2, a_2,..., s_T, a_T$ là chuỗi sự kiện từ state $s_1$ đến state $s_T$. Xác suất xảy ra chuỗi $\\tau$:")]),t._v(" "),e("p",[t._v("[\n\\begin{eqnarray}\np_\\theta(\\tau) &=& p_\\theta(s_1, a_1, s_2, a_2,...s_T, a_T) \\\\\n&=& p(s_1)\\pi_\\theta(a_1|s_1)p(s_2|s_1, a_1)\\pi_\\theta(a_2|s_2)...p(s_{T}|s_{T-1},a_{T-1})\\pi_\\theta(a_T|s_T) \\\\\n&=& p(s_1)\\Pi_{t=1}^{t=T}\\pi_\\theta(a_t|s_t)p(s_{t+1}|s_t, a_t) \\\\\n\\end{eqnarray}\n]")]),t._v(" "),e("p",[t._v("Chúng ta sẽ thấy phân bố xác suất của state $p(s_{t+1}|a_t, s_t)$ bị loại bỏ trong các phương trình về sau.")]),t._v(" "),t._m(8),t._v(" "),e("p",[t._v("$$\n\\begin{eqnarray}\n\\theta^* &=& \\arg\\max_\\theta E_{\\tau\\sim p_\\theta(\\tau)}\\big[r(\\tau)\\big] \\\\\n&=& \\arg\\max_\\theta E_{\\tau\\sim p_\\theta(\\tau)}\\bigg[\\sum_t r(a_t, s_t)\\bigg]\n\\end{eqnarray}\n$$")]),t._v(" "),t._m(9),t._v(" "),e("p",[t._v("$$\n\\begin{eqnarray}\nJ(\\theta) &=& E_{\\tau\\sim p_\\theta(\\tau)}\\bigg[\\sum_t r(a_t, s_t)\\bigg] \\\\\n&=& \\frac{1}{N} \\sum_i\\sum_t r(a_t, s_t)\n\\end{eqnarray}\n$$")]),t._v(" "),t._m(10),t._v(" "),e("p",[t._v("$$\n\\begin{eqnarray}\nJ(\\theta) &=& E_{\\tau\\sim p_\\theta(\\tau)}\\bigg[\\sum_t r(a_t, s_t)\\bigg] \\\\\n&=& \\int p_\\theta(\\tau) r(\\tau) dr\n\\end{eqnarray}\n$$")]),t._v(" "),e("p",[t._v("Tiếp tục xem xét gradient của hàm mục tiêu:")]),t._v(" "),t._m(11),t._v(" "),e("p",[t._v("Tiếp tục phân tích hàm $\\log p_\\theta(\\tau)$, như ta đã có ở trên $p_\\theta(\\tau) = p(s_1)\\Pi_{t=1}^{t=T}\\pi_\\theta(a_t|s_t)p(s_{t+1}|s_t, a_t)$, ta có:\n$$\n\\begin{eqnarray}\n\\log p_\\theta(\\tau) = \\log p(s_1) + \\sum_{t=1}^{t=T}\\log \\pi_\\theta(a_t|s_t) + \\sum_{t=1}^{t=T}\\log p(s_{t+1}|s_t, a_t)\n\\end{eqnarray}\n$$")]),t._v(" "),e("p",[t._v("Cuối cùng:\n$$\n\\begin{eqnarray}\n\\nabla_\\theta \\log p_\\theta(\\tau) = \\sum_{t=1}^{t=T}\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\n\\end{eqnarray}\n$$\nKết quả này rất hay vì đạo hàm theo $\\theta$ của hàm $\\log p_\\theta(\\tau)$ đã không còn phụ thuộc vào phân bố xác suất chuyển của state $p(s_{t+1}|a_t, s_t)$, nó chỉ phụ thuộc vào phân bố xác suất của action $a_i$ chúng ta thực hiện trên $s_i$.")]),t._v(" "),e("p",[t._v("Gradient của hàm mục tiêu lúc này:\n$$\n\\begin{eqnarray}\n\\nabla_\\theta J(\\theta) &=&  E_{\\tau\\sim p_\\theta(\\tau)}\\bigg[\\nabla_\\theta \\log p_\\theta(\\tau) r(\\tau)\\bigg] \\\\\n&=& E_{\\tau\\sim p_\\theta(\\tau)}\\bigg[\\sum_{t=1}^{t=T}\\nabla_\\theta \\log\\pi_\\theta(a_t|s_t)\\sum_{t=1}^{t=T} r(a_t, s_t)\\bigg]\n\\end{eqnarray}\n$$")]),t._v(" "),e("p",[t._v("Tương tự, sau khi trải qua $N$ episodes, expectation của hàm gradient này là:\n$$\n\\begin{eqnarray}\n\\nabla_\\theta J(\\theta) &=& \\frac{1}{N}\\sum_{i=1}^{N}\\bigg(\\sum_{t=1}^{t=T}\\nabla_\\theta \\log \\pi_\\theta(a_{i,t}|s_{i,t})\\bigg)\\bigg(\\sum_{t=1}^{t=T} r(a_{i,t}, s_{i,t})\\bigg)\n\\end{eqnarray}\n$$")]),t._v(" "),e("p",[t._v("Cuối cùng, chúng ta update $\\theta$ như dùng gradient ascent:\n$$\n\\begin{eqnarray}\n\\theta \\leftarrow \\theta + \\nabla_\\theta J(\\theta)\n\\end{eqnarray}\n$$")]),t._v(" "),e("h1",[t._v("3 - REINFORCE algorithm")]),t._v(" "),e("p",[t._v("Tổng hợp các kết quả trên ta có thuật toán REINFORCE như dưới đây:")]),t._v(" "),t._m(12),t._v(" "),e("p",[t._v("Bây giờ hãy dừng lại để xem xét gradient của phương trình mục tiêu. Viết ở dạng đỡ rối mắt hơn ta có:")]),t._v(" "),t._m(13),t._v(" "),e("h1",[t._v("4 - Định nghĩa thêm 1 số khái niệm mới")]),t._v(" "),t._m(14),t._v(" "),e("h2",[t._v("4.1 - Bellman Equations")]),t._v(" "),e("p",[t._v("Từ công thức ở trên, ta có:")]),t._v(" "),e("p",[t._v("$$\n\\begin{eqnarray}\nV^\\pi(s_t) &=& E_\\pi\\bigg[G_t|S=s_t\\bigg] \\\\\n&=& E_\\pi\\bigg[\\sum^{\\infty}_{k=0}\\gamma^kR_{k+t+1}|S=s_t\\bigg] \\\\\n\\end{eqnarray}\n$$")]),t._v(" "),e("p",[t._v("Lấy reward $R_{t+1}$ nhận được khi chuyển từ state $s_t$ sang $s_{t+1}$ ra ngoài dấu $\\sum$, ta được:")]),t._v(" "),e("p",[t._v("$$\n\\begin{eqnarray}\nE_\\pi\\bigg[R_{t+1} + \\gamma\\sum^{\\infty}_{k=0}\\gamma^kR_{k+t+2}|S=s_t\\bigg] &=& E_\\pi[R_{t+1}|S=s_t] + \\gamma E_\\pi\\bigg[\\sum^{\\infty}_{k=0}\\gamma^kR_{k+t+2}|S=s_t\\bigg]\n\\end{eqnarray}\n$$")]),t._v(" "),e("p",[t._v("Khai triển expected value của 2 cụm ở trên ta có:")]),t._v(" "),e("p",[t._v("$$\n\\begin{eqnarray}\nE_\\pi\\bigg[R_{t+1}|S=s_t\\bigg]=\\sum_a \\pi(s_t,a) \\sum_{s_{t+1}} p(s_{t+1}|s_t, a)R(s_{t+1}|s_t, a)\n\\end{eqnarray}\n$$\nMà:")]),t._v(" "),e("p",[t._v("$$\n\\begin{eqnarray}\n\\gamma E_\\pi\\bigg[\\sum^{\\infty}_{k=0}\\gamma^kR_{k+t+2}|S=s_t\\bigg] = \\sum_a \\pi(s_t,a) \\sum_{s_{t+1}} p(s_{t+1}|s_t, a)\\gamma E_\\pi\\bigg[\\sum^\\infty_{k=0} \\gamma^k R_{t+k+2} | S = s_{t+1}\\bigg]\n\\end{eqnarray}\n$$\nTa có:\n$$\n\\begin{eqnarray}\nV^\\pi(s_t) = \\sum_a \\pi(s_t,a) \\sum_{s_{t+1}} p(s_{t+1}|s_t, a)\\Bigg[R(s_{t+1}|s_t, a) + \\gamma E_\\pi\\bigg[\\sum^\\infty_{k=0} \\gamma^k R_{t+k+2} | S = s_{t+1} \\bigg]\\Bigg]\n\\end{eqnarray}\n$$\nĐể ý rằng:\n$$\n\\begin{eqnarray}\nE_\\pi\\bigg[\\sum^\\infty_{k=0} \\gamma^k R_{t+k+2} | S = s_{t+1}\\bigg] = V^\\pi(s_{t+1})\n\\end{eqnarray}\n$$")]),t._v(" "),e("p",[t._v("Cuối cùng ta có:\n$$\n\\begin{eqnarray}\nV^\\pi(s_t) = \\sum_a \\pi(s_t,a) \\sum_{s_{t+1}} p(s_{t+1}|s_t, a)\\bigg[R(s_{t+1}|s_t, a) + \\gamma  V^\\pi(s_{t+1})\\bigg]\n\\end{eqnarray}\n$$\nTương tự với:\n$$\n\\begin{eqnarray}\nQ^\\pi(s_t, a_t) = \\sum_{s_{t+1}} p(s_{t+1}|s_t, a)\\bigg[R(s_{t+1}|s_t, a) + \\gamma \\sum_{a_{t+1}} \\pi(s_{t+1}, a_{t+1}) Q^\\pi (s_{t+1}, a_{t+1}) \\bigg]\n\\end{eqnarray}\n$$\nMà theo quan hệ giữa $V^\\pi$ và $Q^\\pi$ ở trên, thì ta lại có:\n$$\n\\begin{eqnarray}\n\\sum_{a_{t+1}} \\pi(s_{t+1}, a_{t+1}) Q^\\pi (s_{t+1}, a_{t+1}) = V^\\pi(s_{t+1})\n\\end{eqnarray}\n$$\nDo đó:\n$$\n\\begin{eqnarray}\nQ^\\pi(s_t, a_t) = \\sum_{s_{t+1}} p(s_{t+1}|s_t, a_t)\\bigg[R(s_{t+1}|s_t, a_t) + \\gamma  V^\\pi(s_{t+1}) \\bigg]\n\\end{eqnarray}\n$$")]),t._v(" "),e("p",[t._v("Tất cả những biến đổi ở trên cho thấy ta có thể biểu diễn giá trị của $Q^\\pi$ và $V^\\pi$ tại state $s_t$ với state $s_{t+1}$. Như vậy, nếu biết được giá trị tại state $s_{t+1}$, ta có thể dễ dàng tính toán được giá trị tại $s_t$. Tóm gọn thì ta có 2 phương trình sau:\n$$\n\\begin{eqnarray}\nV^\\pi(s_t) &=& \\sum_a \\pi(s_t,a) \\sum_{s_{t+1}} p(s_{t+1}|s_t, a)\\bigg[R(s_{t+1}|s_t, a) + \\gamma  V^\\pi(s_{t+1})\\bigg] \\\\\nQ^\\pi(s_t, a_t) &=& \\sum_{s_{t+1}} p(s_{t+1}|s_t, a_t)\\bigg[R(s_{t+1}|s_t, a_t) + \\gamma  V^\\pi(s_{t+1}) \\bigg]\n\\end{eqnarray}\n$$")]),t._v(" "),e("p",[t._v("Trở lại với gradient hàm mục tiêu, bây giờ ta có:\n$$\n\\begin{eqnarray}\n\\nabla_\\theta J(\\theta) =  E_{\\tau\\sim p_\\theta(\\tau), a\\sim\\pi_\\theta}\\bigg[\\nabla_\\theta \\log\\pi_\\theta(a|s)Q^\\pi(s,a)\\bigg]\n\\end{eqnarray}\n$$")]),t._v(" "),t._m(15),t._v(" "),t._m(16),t._v(" "),t._m(17),t._v(" "),e("h1",[t._v("6 - Stochastic Actor-Critic")]),t._v(" "),t._m(18),t._v(" "),e("p",[t._v("Xem xét gradient của hàm mục tiêu mà ta đã có ở phần trên:\n$$\n\\begin{eqnarray}\n\\nabla_\\theta J(\\theta) =  E_{\\tau\\sim p_\\theta(\\tau), a\\sim\\pi_\\theta}\\bigg[\\nabla_\\theta \\log\\pi_\\theta(a|s)\\Big(Q^\\pi(s,a)-V^\\pi(s)\\Big)\\bigg]\n\\end{eqnarray}\n$$\nTừ Bellman Equation ở trên ta có quan hệ giữa $Q^\\pi$ và $V^\\pi$, lúc này hàm mục tiêu trở thành:\n$$\n\\begin{eqnarray}\n\\nabla_\\theta J(\\theta) =  E_{\\tau\\sim p_\\theta(\\tau), a\\sim \\pi_\\theta}\\bigg[\\nabla_\\theta \\log\\pi_\\theta(a|s)\\Big(R + \\gamma V^\\pi(s_{t+1})- V^\\pi(s)\\Big)\\bigg]\n\\end{eqnarray}\n$$")]),t._v(" "),t._m(19),t._v(" "),e("h1",[t._v("7 - Actor-Critic Algorithm")]),t._v(" "),e("p",[t._v("Từ thuật toán REINFORCE, bây giờ chúng ta sử dụng thêm hàm xấp xỉ cho value function $V_\\phi$, thay đổi một chút ta có như sau:")]),t._v(" "),t._m(20),t._v(" "),e("h1",[t._v("8 - Từ Stochastic Actor-Critic tới Q-Learning")]),t._v(" "),t._m(21),t._v(" "),t._m(22),t._v(" "),e("br"),t._v("\nNhư vậy, với một policy $\\pi$, ta luôn có thể áp dụng policy $\\pi'$ trên đó để được một policy ít nhất là bằng hoặc tốt hơn."),e("br"),t._v("\nTa có thuật toán bây giờ có thể viết như sau:"),e("br"),t._v("\n1. Đánh giá $A^\\pi(s,a)$ với các action $a$ khác nhau"),e("br"),t._v("\n2. Tối ưu $\\pi \\leftarrow \\pi'$\n"),t._m(23),t._v(" "),t._m(24),t._v(" "),e("p",[t._v("Đến đây thì thực sự ta không cần quan tâm đến policy nữa, mà bước 2 có thể viết lại thành:")]),t._v(" "),t._m(25),t._v(" "),e("p",[t._v("Nếu xử dụng một hàm xấp xỉ cho $V_\\phi(s)$, ta có thuật toán như sau:")]),t._v(" "),t._m(26),t._v(" "),e("p",[t._v("Thuật toán này không ổn, ở bước 1 ta cần có reward $r(s, a)$ ứng với mỗi action $a$ khác nhau, như vậy ta cần nhiều simulation tại 1 state $s$. Ta có thể làm tương tự với $Q(s,a)$ thay vì $V(s)$.")]),t._v(" "),t._m(27),t._v(" "),e("p",[t._v("Đây chính là thuật toán Q-Learning. Để ý rằng, reward $r$ ở trên không phụ thuộc vào state transition và cũng không\nphụ thuộc vào policy $\\pi$ dùng để sinh ra sample, nên ta chỉ cần có sample $(s, a, r, s')$ là có thể cải thiện được\npolicy mà không cần biết nó được sinh ra từ policy nào. Do đó, thuật toán này được gọi là off-policy. Sau này, chúng\nta sẽ có các thuật toán on-policy, các thuật toán on-policy thì phải dựa vào sample được sinh ra tại policy hiện tại\nđể có thể cải thiện policy mới.")]),t._v(" "),e("p",[t._v("Ta có thuật toán Online Q-Learning như sau:")]),t._v(" "),t._m(28),t._v(" "),t._m(29),t._v(" "),e("p",[t._v("Tóm lại, để thuật toán ổn định và hội tụ, ta cần:")]),t._v(" "),t._m(30),t._v(" "),e("p",[t._v("Thuật toán bây giờ được viết như sau:")]),t._v(" "),t._m(31),t._v(" "),e("h1",[t._v("9 - Từ Deep Q-Network đến Deep Deterministic Policy Gradient")]),t._v(" "),e("p",[t._v("Thuật toán DQN đã thành công trong việc sấp xỉ Q-value, nhưng có một hạn chế ở bước 3: chúng ta cần đánh giá $Q_{\\phi'}$ với tất cả các action khác nhau để chọn ra $Q$ lớn nhất. Với discrete action space như các game, khi mà action chỉ là các nút bấm lên xuống qua lại, số lượng action là hữu hạn, điều này có thể thực hiện được. Tuy nhiên, với continuous action space, ví dụ action có thể trong khoảng từ 0 đến 1, ta cần có một cách tiếp cận khác.")]),t._v(" "),e("p",[t._v("Cách ta có thể nghĩ đến đó là tìm cách phân nhỏ action space, phân nhỏ continuous space thành các khoảng nhỏ, ví dụ như từ 0 đến 1,  ta có thể phân ra làm 5 đến 10 khoảng giá trị có thể rơi vào. Một cách khác đó là sample ngẫu nhiên các action khác nhau trong khoảng cho phép với cùng state $s$ và chọn ra giá trị $Q(s,a)$ lớn nhất.")]),t._v(" "),t._m(32),t._v(" "),e("p",[t._v("Bây giờ, nếu như ta có thêm một hàm xấp xỉ $\\mu_\\theta(s) = \\arg max_a Q_\\phi(s,a)$, bây giờ ta tìm bộ số $\\theta$ sao cho: $\\theta \\leftarrow  \\arg max_\\theta Q_\\phi(s,\\mu_\\theta(s))$. Phép tối ưu này xem xét sự thay đổi của $Q_\\phi$ theo biến $\\theta$. Ta có thể tính được sự thay đổi này dựa trên chain rule như sau:\n$\\frac{dQ_\\phi}{d\\theta} = \\frac{dQ_\\phi}{d\\mu} \\frac{d\\mu}{d\\theta}$.")]),t._v(" "),e("p",[t._v("Để ý rằng, $\\mu_\\theta(s)$ là một Deterministic Policy, chính vì vậy phương pháp này gọi là Deep Deterministic Policy Gradient.")]),t._v(" "),e("p",[t._v("Thuật toán DDPG như sau:")]),t._v(" "),t._m(33),t._v(" "),t._m(34),t._v(" "),e("h1",[t._v("10 - Kết bài")]),t._v(" "),e("p",[t._v("Như vậy, chúng ta đã di qua phần cơ bản từ thuật toán Policy Gradient đến DQN và DDPG.")]),t._v(" "),e("p",[t._v("Tóm gọn:")]),t._v(" "),t._m(35)])},staticRenderFns:[function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("p",[t._v("Reinforcement Learning hay học củng cố/tăng cường, là lĩnh vực liên quan\nđến việc dạy cho máy "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("agent"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(" thực hiện tốt một nhiệm vụ "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("task"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(" bằng cách\ntương tác với môi trường "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("environment"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(" thông qua hành động "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("action"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(" và\nnhận được phần thưởng "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("reward"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(". Cách học này rất giống với cách con người\nhọc từ môi trường bằng cách thử sai. Lấy ví dụ 1 đứa vào mùa đông đến gần\nlửa thì thấy ấm, đứa trẻ sẽ có xu hướng đến gần lửa nhiều hơn\n"),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("vì nhận được phần thưởng là ấm áp"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(",\nnhưng chạm vào lửa thì nóng, đứa trẻ sẽ có xu hướng tránh chạm vào lửa "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("vì\nbị bỏng tay"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(".")])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("Trong ví dụ trên, phần thưởng xuất hiện ngay, việc điều chỉnh hành động là\ntương đối dễ. Tuy nhiên, trong các tình huống phức tạp hơn khi mà phần thưởng\nở xa trong tương lai, điều này trở nên phức tạp hơn. Làm sao để đạt được tổng\nphần thưởng cao nhất trong suốt cả quá trình? Reinforcement Learning "),a("span",{staticClass:"tex2jax_ignore"},[this._v("(")]),this._v("RL"),a("span",{staticClass:"tex2jax_ignore"},[this._v(")")]),this._v(" là\ncác thuật toán để giải bài toán tối ưu này.")])},function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("ul",[e("li",[e("em",[t._v("Environment")]),t._v(" "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("môi trường"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(": là không gian mà máy tương tác.")]),t._v(" "),e("li",[e("em",[t._v("Agent")]),t._v(" "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("máy"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(": máy quan sát môi trường và sinh ra hành động tương ứng.")]),t._v(" "),e("li",[e("em",[t._v("Policy")]),t._v(" "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("chiến thuật"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(": máy sẽ theo chiến thuật như thế nào để đạt được mục đích.")]),t._v(" "),e("li",[e("em",[t._v("Reward")]),t._v(" "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("phần thưởng"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(": phần thưởng tương ứng từ môi trường mà máy nhận được khi thực hiện một hành động.")]),t._v(" "),e("li",[e("em",[t._v("State")]),t._v(" "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("trạng thái"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(": trạng thái của môi trường mà máy nhận được.")]),t._v(" "),e("li",[e("em",[t._v("Episode")]),t._v(" "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("tập"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(": một chuỗi các trạng thái và hành động cho đến trạng thái kết thúc $s_1,a_1,s_2,a_2,...s_T, a_T$")]),t._v(" "),e("li",[e("em",[t._v("Accumulative Reward")]),t._v(" "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("phần thưởng tích lũy"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(": tổng phần thưởng tích lũy từ 1 state đến state cuối cùng.")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("div",{staticStyle:{width:"image width px","font-size":"80%","text-align":"center"}},[a("img",{attrs:{src:"https://i.imgur.com/nIUdsIm.jpg",align:"center"}}),this._v(" "),a("div",[this._v("Hình 1: Vòng lặp tương tác giữa agent và environment.")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("Xem ví dụ dưới đây từ openAI Gym, environment có tên\n"),a("a",{attrs:{href:"https://github.com/openai/gym/wiki/MountainCarContinuous-v0"}},[this._v("MountaincontinuousCar-v0")]),this._v(".")])},function(){var t=this.$createElement,a=this._self._c||t;return a("div",{staticStyle:{width:"image width px","font-size":"80%","text-align":"center"}},[a("img",{staticStyle:{"padding-bottom":"0.5em"},attrs:{src:"https://i.imgur.com/yGWmDei.jpg",alt:"MountaincontinuousCar-v0"}}),this._v(" "),a("div",[this._v("Hình 2: Một hình ảnh từ MountaincontinuousCar-v0.")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("div",{staticStyle:{width:"image width px","font-size":"80%","text-align":"center"}},[a("img",{attrs:{src:"https://laptrinhcuocsong.com/images/game-hung-trung.png",align:"center"}}),this._v(" "),a("div",[this._v("Hình 3: Trò chơi Hứng Trứng.")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("Trong game Hứng Trứng, giả sử ta có 3 action: qua trái, qua phải, đứng yên.\nTương ứng với state $s$ hiện tại "),a("span",{staticClass:"tex2jax_ignore"},[this._v("(")]),this._v("vị trí của thùng hứng, vị trí của trứng rơi so với thùng,\ntốc độ của bia rơi..."),a("span",{staticClass:"tex2jax_ignore"},[this._v(")")]),this._v(" ta sẽ có 1 phân bố xác suất của 3 action tương ứng,\nví dụ $[0.1, 0.3, 0.5]$. Tổng xác suất của tất cả các hành động tại state $s$ bằng $1$, ta có: $\\sum_{a}\\pi_\\theta(a|s) = 1$.\nGọi $p(s_{t+1}|a_t, s_t)$ là hàm phân bố xác suất của state tiếp theo khi agent tại state $s$ và thực hiện action $a$.")])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[a("strong",[this._v("Mục tiêu của reinforcement learning là tìm $\\theta$ sao cho:")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("Từ công thức ta có thể thấy $\\theta^*$ là bộ tham số sao cho expectation của accumulative reward từ rất\nnhiều các mẫu $\\tau$ khác nhau có được từ việc thực thi theo policy $\\pi_\\theta$ là lớn nhất."),a("br"),this._v("\nSau khi trải qua $N$ episodes khác nhau ta thu được $N$ mẫu $\\tau$ khác nhau. Hàm số mục tiêu của bài toán lúc này:")])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("$J(\\theta)$ chính là trung bình cộng của accumulative reward của episodes khác nhau."),a("br"),this._v("\nChúng ta cũng có thể biểu diễn $J(\\theta)$ theo phân bố của xác suất phân bố $p_\\theta(\\tau)$ như sau:")])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("$$\n\\begin{eqnarray}\n\\nabla_\\theta J(\\theta) &=& E_{\\tau\\sim p_\\theta(\\tau)}\\bigg[\\sum_t r(a_t, s_t)\\bigg] \\\\\n&=& \\int \\nabla_\\theta p_\\theta(\\tau) r(\\tau) dr\n\\end{eqnarray}\n$$\nMà chúng ta lại có:\n$$\n\\begin{eqnarray}\n\\nabla_\\theta p_\\theta(\\tau) &=&  p_\\theta(\\tau) \\frac{\\nabla_\\theta p_\\theta(\\tau)} {p_\\theta(\\tau)} \\\\\n&=& p_\\theta(\\tau)\\nabla_\\theta \\log p_\\theta(\\tau)\n\\end{eqnarray}\n$$\n"),a("strong",[this._v("Lưu ý")]),this._v(" trick này rất thường xuyên được sử dụng, do đó:\n$$\n\\begin{eqnarray}\n\\nabla_\\theta J(\\theta) &=& \\int p_\\theta(\\tau) \\nabla_\\theta \\log p_\\theta(\\tau) r(\\tau) dr \\\\\n&=& E_{\\tau\\sim p_\\theta(\\tau)}\\bigg[\\nabla_\\theta \\log p_\\theta(\\tau) r(\\tau)\\bigg]\n\\end{eqnarray}\n$$")])},function(){var t=this.$createElement,a=this._self._c||t;return a("ol",[a("li",[this._v("Lấy 1 tập N chuỗi {$\\tau^i$} dựa theo policy $\\pi_\\theta$")]),this._v(" "),a("li",[this._v("Tính gradient: $\\nabla_\\theta J(\\theta) = \\frac{1}{N}\\sum_{i=1}^{N}\\bigg(\\sum_{t=1}^{t=T}\\nabla_\\theta \\log \\pi_\\theta(a_{i,t}|s_{i,t})\\bigg)\\bigg(\\sum_{t=1}^{t=T} r(a_{i,t}, s_{i,t})\\bigg) $")]),this._v(" "),a("li",[this._v("Update $\\theta \\leftarrow \\theta + \\nabla_\\theta J(\\theta)$")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("$$\n\\begin{eqnarray}\n\\nabla_\\theta J(\\theta) = \\frac{1}{N}\\sum_{i=1}^{N}\\nabla_\\theta \\log \\pi_\\theta(\\tau_i)r(\\tau_i)\n\\end{eqnarray}\n$$\nĐây chính là maximum likelihood estimation "),a("a",{attrs:{href:"https://vi.wikipedia.org/wiki/H%E1%BB%A3p_l%C3%BD_c%E1%BB%B1c_%C4%91%E1%BA%A1i"}},[this._v("MLE")]),this._v(" tích với accumulative reward.\nViệc tối ưu hàm mục tiêu cũng đồng nghĩa với việc tăng xác suất để đi theo chuỗi $\\tau$ cho accumulative reward cao.")])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("$V^\\pi(s)$: accumulative reward mong đợi tại state $s$ nếu đi theo policy $\\pi$."),a("br"),this._v("\n$Q^\\pi(s,a)$: accumulative reward mong đợi nếu thực hiện action $a$ tại state $s$ nếu đi theo policy $\\pi$."),a("br"),this._v("\nQuan hệ giữa $V^\\pi(s)$ và $Q^\\pi(s,a)$: $V^\\pi(s) = \\sum_{a \\in A}\\pi_\\theta(s,a)Q^\\pi(s,a)$ - điều này là hợp lý vì $\\pi_\\theta(s,a)$ là xác suất thực hiện action $a$ tại $s$."),a("br"),this._v("\nTa cũng có như sau:\n$$\n\\begin{eqnarray}\nV^\\pi(s_t) &=& E_\\pi[G_t | S=s_t] \\\\\nQ^\\pi(s_t,a_t) &=& E_\\pi[G_t|S=s_t, A=a_t]\n\\end{eqnarray}\n$$\nTrong đó:"),a("br"),this._v("\n$G_t=\\sum^{\\infty}_{k=0}\\gamma^kR_{k+t+1}$: tổng tất cả các reward nhận được kể từ state $s_t$ đến tương lai, với đại lượng $\\gamma$ gọi là discount factor: $0 < \\gamma < 1$. Càng xa hiện tại, reward càng bị discount nhiều, agent quan tâm nhiều hơn đến reward ở gần hơn là ở xa.")])},function(){var t=this.$createElement,a=this._self._c||t;return a("h1",[this._v("5 - Advantage "),a("span",{staticClass:"tex2jax_ignore"},[this._v("(")]),this._v("lợi thế"),a("span",{staticClass:"tex2jax_ignore"},[this._v(")")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("$$\n\\begin{eqnarray}\n\\nabla_\\theta J(\\theta) =  E_{\\tau\\sim p_\\theta(\\tau), a\\sim\\pi_\\theta}\\bigg[\\nabla_\\theta \\log\\pi_\\theta(a|s)Q^\\pi(s,a)\\bigg]\n\\end{eqnarray}\n$$\nGradient của hàm mục tiêu cho thấy việc tăng khả năng thực hiện action $a$ nếu nhận được $Q^\\pi(s,a)$ cao. Giả sử agent ở tại state $s$, việc ở tại state $s$ là đã có lợi cho agent rồi, thực hiện action $a$ nào cũng cho ra giá trị $Q^\\pi(s,a)$ cao thì ta không thể phân tách "),a("span",{staticClass:"tex2jax_ignore"},[this._v("(")]),this._v("discriminate"),a("span",{staticClass:"tex2jax_ignore"},[this._v(")")]),this._v(" các action $a$ với nhau và từ đó không biết được action $a$ nào là tối ưu. Do đó ta cần có 1 baseline để so sánh các giá trị $Q^\\pi(s,a)$."),a("br"),this._v("\nNhư trong phần 4, ta có $V^\\pi(s)$ là expectation của accumulative reward tại state $s$, không quan trọng tại $s$ agent thực hiện action gì, chúng ta mong đợi 1 accumulative reward là $V^\\pi(s)$.\nDo đó, 1 action $a_m$ được đánh giá là tệ nếu như $Q^\\pi(s,a_m)$ < $V^\\pi(s)$ và 1 action $a_n$ được đánh giá là tốt nếu như $Q^\\pi(s,a_n)$ > $V^\\pi(s)$. Từ đây ta có được 1 baseline để so sánh $Q^\\pi(s,a)$ đó là $V^\\pi(s)$. Gradient của objective function bây giờ có thể viết lại được như sau:\n$$\n\\begin{eqnarray}\n\\nabla_\\theta J(\\theta) =  E_{\\tau\\sim p_\\theta(\\tau), a\\sim\\pi_\\theta}\\bigg[\\nabla_\\theta \\log\\pi_\\theta(a|s)\\Big(Q^\\pi(s,a)-V^\\pi(s)\\Big)\\bigg]\n\\end{eqnarray}\n$$")])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("Nếu $Q^\\pi(s,a)-V^\\pi(s) < 0$, 2 gradient ngược dấu với nhau, tối ưu hàm mục tiêu sẽ làm giảm gradient của việc thực thi hành động $a$ tại $s$."),a("br"),this._v("\nTa gọi $A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)$ là Advantage của action $a$ tại state $s$.")])},function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("p",[t._v("Stochastic Actor "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("ngẫu nhiên Actor"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(" ý chỉ policy $\\pi_\\theta(a|s)$ là một hàm phân phối xác suất của action $a$ tại $s$. Ta gọi Stochastic Actor để phân biệt với Deterministic Actor "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("hay Deterministic Policy"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(" mang ý chỉ policy không còn là một hàm phân phối xác suất của các action $a$ tại $s$, mà dưới $s$ ta chỉ thực hiện chính xác một action nhất định mà thôi $a=\\mu_\\theta(s)$, hay nói cách khác xác suất thực hiện $a$ tại $s$ bây giờ là 1.")])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("Hàm mục tiêu phụ thuộc vào 2 thứ: policy $\\pi_\\theta$ và value function $V^\\pi$. Giả sử ta có một hàm xấp xỉ cho $V^\\pi(s)$ là $V_\\phi(s)$ phụ thuộc vào bộ tham số $\\phi$."),a("br"),this._v("\nTa gọi hàm xấp xỉ cho policy $\\pi_\\theta$ là Actor và hàm xấp xỉ cho value function $V_\\phi$ là Critic.")])},function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("p",[t._v("Batch Actor-Critic:"),e("br"),t._v("\n1. Lấy 1 chuỗi $\\tau$ đến terminal state dựa theo policy $\\pi_\\theta$"),e("br"),t._v("\n2. Fit $V_\\phi$ với $y = \\sum_{i}^{T} r_i$"),e("br"),t._v("\n3. Tính $A(s_t,a_t) = r(s_t, a_t) + \\gamma V_\\phi(s_{t+1}) - V_\\phi(s_{t})$"),e("br"),t._v("\n4. Tính $\\nabla_\\theta J(\\theta) = \\sum_i \\nabla \\log \\pi_\\theta (a_i|s_i) A^\\pi (s_i, a_i)$"),e("br"),t._v("\n5. Update $\\theta \\leftarrow \\theta  + \\alpha \\nabla_\\theta J(\\theta)$"),e("br"),t._v(" "),e("br"),t._v(" "),e("br"),t._v("\nMà ta có ở trên, ta có thể biểu diễn $V_\\phi(s) = r + V_\\phi(s')$ theo Bellman Equation, do đó ta có thể update model mà chỉ cần 1 step về phía trước."),e("br"),t._v("\nOnline Actor-Critic:"),e("br"),t._v("\n1. Dựa theo policy $\\pi_\\theta$, thực hiện 1 action $a \\sim \\pi_\\theta(a|s)$ để có $(s,a,s',r)$"),e("br"),t._v("\n2. Fit $V_\\phi (s)$ với $r + V_\\phi(s')$"),e("br"),t._v("\n3. Tính $A(s_t,a_t) = r(s_t, a_t) + \\gamma V_\\phi(s_{t+1}) - V_\\phi(s_{t})$"),e("br"),t._v("\n4. Tính $\\nabla_\\theta J(\\theta) = \\sum_i \\nabla \\log \\pi_\\theta (a_i|s_i) A (s_i, a_i)$"),e("br"),t._v("\n5. Update $\\theta \\leftarrow \\theta  + \\alpha \\nabla_\\theta J(\\theta)$"),e("br"),t._v(" "),e("br"),t._v(" "),e("br"),t._v("\nNhư vậy, chúng ta cùng lúc update cả 2 hàm xấp xỉ $V_\\phi$ và $\\pi_\\theta$.")])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("Xét một policy như sau:\n$$\n\\begin{eqnarray}\n\\pi'(a_t|s_t) = 1 \\ \\text{if}\\  a_t = \\arg \\max_{a_t} A^\\pi(s_t, a_t)\n\\end{eqnarray}\n$$\nPolicy $\\pi'$ này là một Deterministic Policy: cho trước một policy $\\pi$ và giả sử ta biết được Advantage của các action tại state $s_t$ dưới policy $\\pi$, ta luôn chọn action cho ra giá trị Advantage lớn nhất tại state $s_t$ đó, probability của action đó là 1, tất cả các action khác tại $s_t$ bằng 0.\nPolicy $\\pi'$ sẽ luôn tốt hơn hoặc ít nhất là tương đương với policy $\\pi$. Một policy được đánh giá là tương đương hay tốt hơn khi ta có:\n$V^\\pi(s) \\leq V^{\\pi'} (s) \\forall s \\in S$ : với mọi state $s$ trong miền state $S$, giá trị return  $V^\\pi(s)$ luôn nhỏ hơn hoặc bằng giá trị return $V^{\\pi'} (s)$."),a("br"),this._v("\nVí dụ ta có như sau: tại state $s$, ta có 4 cách đi sang state $s'$ ứng với 4 action và ứng với $A^\\pi_1$, $A^\\pi_2$, $A^\\pi_3$, $A^\\pi_4$. Kể từ state $s'$, ta lại đi theo policy $\\pi$. Từ $s$ sang $s'$, nếu chọn action theo stochastic policy $\\pi$, Expected Advantage là $\\sum_{a \\in A} p(a)A^\\pi_a$, lượng này chắc chắn nhỏ hơn $\\max_a A^\\pi_a$.")])},function(){var t=this.$createElement,a=this._self._c||t;return a("div",{staticStyle:{width:"image width px","font-size":"80%","text-align":"center"}},[a("img",{attrs:{src:"https://i.imgur.com/yMtTahR.jpg",align:"center"}}),this._v(" "),a("div",[this._v("Hình 4: Thay đổi từ trạng thái $s$ sang $s'$.")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("Mà đánh giá $A^\\pi(s,a)$ thì cũng tương đương đánh giá $Q^\\pi(s,a)$ vì $A^\\pi(s,a) =  Q^\\pi(s,a) - V^\\pi(s) = r(s,a)  + \\gamma V^\\pi(s') - V^\\pi(s)$, mà lượng $V^\\pi(s)$ thì lại không đổi giữa các action $a$ tại state $s$."),a("br"),this._v("\nNhư vậy, thuật toán trở thành:")])},function(){var t=this.$createElement,a=this._self._c||t;return a("ol",[a("li",[this._v("Đánh giá $Q^\\pi(s,a) \\leftarrow r(s,a)  + \\gamma V^\\pi(s') $ với các action $a$ khác nhau")]),this._v(" "),a("li",[this._v("Tối ưu $\\pi \\leftarrow \\pi'$ : chọn ra action cho $A$ cao nhất, hay cũng chính là $Q$")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("ol",[a("li",[this._v("Đánh giá $Q^\\pi(s,a) \\leftarrow r(s,a)  + \\gamma V^\\pi(s') $ với các action $a$ khác nhau")]),this._v(" "),a("li",[this._v("$V^\\pi(s) \\leftarrow \\max_a Q^\\pi(s,a)$")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("ol",[a("li",[this._v("Đánh giá $V^\\pi(s) \\leftarrow \\max_a \\big(r(s,a)  + \\gamma V^\\pi(s')\\big)$")]),this._v(" "),a("li",[this._v("$\\phi \\leftarrow \\arg min_\\phi \\big(V^\\pi(s) - V_\\phi(s)\\big)^2$")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("ol",[a("li",[this._v("Đánh giá $y_i \\leftarrow r(s,a_i)  + \\gamma \\max_{a'} Q_\\phi(s', a') $")]),this._v(" "),a("li",[this._v("$\\phi \\leftarrow \\arg min_\\phi \\big(Q_\\phi(s, a_i) - y_i\\big)^2$ $(*)$")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("ol",[a("li",[this._v("Thực hiện action $a$ để có $(s, a, s', r)$")]),this._v(" "),a("li",[this._v("Đánh giá $y_i \\leftarrow r(s,a_i)  + \\gamma max_{a'} Q_\\phi(s', a') $")]),this._v(" "),a("li",[this._v("$\\phi \\leftarrow \\phi - \\alpha \\frac{dQ_\\phi}{d\\phi}(s,a) \\big(Q_\\phi(s, a_i) - y_i\\big)$")])])},function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("p",[t._v("Các bạn để ý bước 3, đó có phải là gradient descent như ở trên chỗ mình đánh dấu $(*)$ không? Không, thực ra chúng ta đã lờ đi phần thay đổi của $y_i$ theo $\\phi$ "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("$y_i$ cũng phụ thuộc vào $\\phi$"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(". Như vậy, mỗi khi update $\\phi$ theo thuật toán này, thì giá trị của mục tiêu $y_i$ cũng bị thay đổi theo! Mục tiêu luôn thay đổi khi ta cố gắng tiến gần lại nó, điều này làm cho thuật toán trở nên không ổn định."),e("br"),t._v("\nĐể giải quyết điều này, ta cần một hàm xấp xỉ khác gọi là target network, khác với train network chúng ta vẫn chạy. Target network sẽ được giữ cố định để tính $y$ và update dần dần."),e("br"),t._v("\nMột vấn đề khác là các sample sinh ra liên tục nên nó rất liên quan "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("correlation"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(" với nhau. Thuật toán trên cũng giống như Supervised Learning - ta map Q-value với giá trị mục tiêu, ta muốn các sample là độc lập "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("i.i.d"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(" với nhau. Để phá vỡ correlation giữa các sample, ta có thể dùng 1 experience buffer: 1 list chứa rất nhiều sample $(s,a,r,s')$ khác nhau, và ta chọn ngẫu nhiên 1 batch từ buffer để train thuật toán.")])},function(){var t=this.$createElement,a=this._self._c||t;return a("ul",[a("li",[this._v("Hàm target network riêng biệt, gọi đó là $Q_{\\phi'}$.")]),this._v(" "),a("li",[this._v("Experience buffer.")])])},function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("ol",[e("li",[t._v("Thực hiện action $a_i$ để có $(s_i, a_i, s_i', r_i)$ và bỏ nó vào buffer.")]),t._v(" "),e("li",[t._v("Lấy ngẫu nhiên 1 batch $N$ sample từ buffer $(s_i, a_i, s_i', r_i)$.")]),t._v(" "),e("li",[t._v("Đánh giá $y_i \\leftarrow r(s,a_i)  + \\gamma max_{a'} Q_{\\phi'}(s', a')$ "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("dùng target network ở đây"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")])]),t._v(" "),e("li",[t._v("$\\phi \\leftarrow \\phi - \\alpha \\frac{1}{N}\\sum_i^N \\frac{dQ_\\phi}{d\\phi}(s,a_i) \\big(Q_\\phi(s, a_i) - y_i\\big)$")]),t._v(" "),e("li",[t._v("Update target network $\\phi' \\leftarrow (1-\\tau)\\phi' + \\tau \\phi$ "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("sử dụng $\\tau %$ của train network mới để update target network"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(" "),e("br"),t._v(" "),e("br"),t._v("\nThuật toán này chính là Deep Q-Network "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("DQN"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(".")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("p",[this._v("Deep Deterministic Policy Gradient "),a("span",{staticClass:"tex2jax_ignore"},[this._v("(")]),this._v("DDPG"),a("span",{staticClass:"tex2jax_ignore"},[this._v(")")]),this._v(" có một cách tiếp cận tinh tế như sau, để ý rằng:\n$$\n\\begin{eqnarray}\nmax_{a} Q_{\\phi}(s, a) = Q_\\phi\\big(s, \\arg max_a Q_\\phi(s,a)\\big)\n\\end{eqnarray}\n$$")])},function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("ol",[e("li",[t._v("Thực hiện action $a_i$ để có $(s_i, a_i, s_i', r_i)$ và bỏ nó vào buffer.")]),t._v(" "),e("li",[t._v("Lấy ngẫu nhiên 1 batch $N$ sample từ buffer $(s_i, a_i, s_i', r_i)$.")]),t._v(" "),e("li",[t._v("Đánh giá $y_i \\leftarrow r(s,a_i)  + \\gamma Q_{\\phi'}\\big(s', \\mu_{\\theta'}(s')\\big)$ "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("dùng cả policy và Q target network ở đây"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")])]),t._v(" "),e("li",[t._v("$\\phi \\leftarrow \\phi - \\alpha \\frac{1}{N}\\sum_i^N \\frac{dQ_\\phi}{d\\phi}(s,a_i) \\big(Q_\\phi(s, a_i) - y_i\\big)$")]),t._v(" "),e("li",[t._v("$\\theta \\leftarrow \\theta - \\beta \\frac{1}{N}\\sum_i^N \\frac{d\\mu_\\theta}{d\\theta}(s) \\frac{dQ_\\phi}{da}(s, a_i)$")]),t._v(" "),e("li",[t._v("Update target network $\\phi' \\leftarrow (1-\\tau)\\phi' + \\tau \\phi$ và $\\theta' \\leftarrow (1-\\tau)\\theta' + \\tau \\theta$\n"),e("br"),t._v(" "),e("br"),t._v("\nLưu ý trong implement DDPG:")])])},function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("ul",[e("li",[t._v("Vì action trong DDPG luôn là deterministic, do đó để có thể khám phá môi trường "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("ta không muốn agent luôn khai khác chỉ 1 đường đi tốt nhất trong những đường đi mà nó biết, có thể có đường đi khác tốt hơn mà nó chưa biết"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(", chúng ta sẽ thêm vào action một lượng noise nhỏ vào action.\nLượng noise này trong "),e("a",{attrs:{href:"https://arxiv.org/abs/1509.02971"}},[t._v("bài báo gốc")]),t._v(" là một stochastic process có tên Ornstein–Uhlenbeck process "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("OU process"),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v(".\nNgười viết chọn process này vì khi làm thí nghiệm cho kết quả tôt, tuy nhiên một vài thí nghiệm khác sử dụng những noise khác như là 1\nGaussian Noise thì cũng cho kết quả tương đương.")]),t._v(" "),e("li",[t._v("Implementation của action noise trong các thư viện như "),e("a",{attrs:{href:"https://github.com/keras-rl/keras-rl"}},[t._v("keras-rl")]),t._v(" là OU process, tuy nhiên khi chạy thử thì noise này không decay theo thời gian. Mà chúng ta cần noise lớn lúc ban đầu "),e("span",{staticClass:"tex2jax_ignore"},[t._v("(")]),t._v("để khám phá môi trường nhiều"),e("span",{staticClass:"tex2jax_ignore"},[t._v(")")]),t._v(" sau đó giảm dần khi đã trải qua nhiều episode.\nViệc này có thể thực hiện nếu trước khi thêm noise vào action, ta nhân nó với một lượng epsilon, mà epsilon giảm về xấp xỉ 0 theo thời gian.")]),t._v(" "),e("li",[t._v("Ngoài việc thêm noise vào action trước khi thực hiện nó trên environment, người ta còn có thể thêm Gaussian Noise vào các nút mạng trong actor network.\nBài báo reference "),e("a",{attrs:{href:"https://openai.com/blog/better-exploration-with-parameter-noise/"}},[t._v("tại đây")]),t._v(". Bạn đọc có thể tìm thấy implementation của actor network noise tại thư viện "),e("a",{attrs:{href:"https://stable-baselines.readthedocs.io/en/master/modules/ddpg.html#action-and-parameters-noise"}},[t._v("stable baselines")]),t._v(".")])])},function(){var t=this.$createElement,a=this._self._c||t;return a("ul",[a("li",[this._v("Policy Gradient là một thuật toán on-policy và là một stochastic policy.")]),this._v(" "),a("li",[this._v("Q-learning, DQN, DDPG là các thuật toán off-policy và là deterministic policy.")]),this._v(" "),a("li",[this._v("Luôn có một deterministic policy tốt hơn một stochastic policy hiện có.")])])}]},n=e("VU/8")(null,i,!1,null,null,null);a.default=n.exports}},["NHnr"]);
//# sourceMappingURL=app.006a6c3185467785515e.js.map